[["Map",1,2,9,10,169,170,180,181,305,306],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.13.7","content-config-digest","5fe9f1c0c5c513d6","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"http://examplesite.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[[null,{\"themes\":\"andromeeda\",\"shiki\":{\"wrap\":false,\"langs\":[]}}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,29,30,44,45,63,64,78,79,95,96,109,110,125,126,140,141,154,155],"turn-the-parking-lots-to-gardens",{"id":11,"data":13,"body":24,"filePath":25,"digest":26,"legacyId":27,"deferredRender":28},{"title":14,"description":15,"image":16,"date":17,"author":18,"categories":19,"tags":22},"Solar Punk Futures: turn the parking lots into gardens/housing","I would like to talk about an imaginative future. While apartment hunting and walking through downtown, it struck me how much space is used for parking.","assets/blog/Firefly_Parking-lots-turned-into-vertical-gardens-in-a-dense-city.-Solar-punk._art_49187-2.webp",["Date","2023-03-01T05:00:00.000Z"],"Edward Deaver",[20,21],"civic","urban-planning",[23],"urbanism","Note: I'm not an urban planner / architect, just someone with a blog. And this is a speculative post, I haven't dived into the GIS data for Syracuse parking lots fully yet.\n\nI would like to talk about an imaginative future. I'm from Syracuse NY, a city designed around the car which has resulted in many parking lots in the city center. Syracuse is on a quest to increase in-fill housing and raise it's density though, so positive thoughts that this culture will change. While apartment hunting and walking through downtown, it struck me how much space is used for parking.\n\nThe idea of turning parking garages into parking garages into housing is relatively new but exists at least in the R\\&D of some firms. From my reading the issues are typically around parking requirements that exist.\n\n[Parking Garage Conversions to Housing | Park House | KTGY Architecture + Planning](https://ktgy.com/work/park-house/?ref=edwarddeaver.me)\n\n[A parking structure ideal for conversion into housing residential units depends on the floor to floor height, proximity to existing buildings, (more)](https://ktgy.com/work/park-house/?ref=edwarddeaver.me)\n\n[KTGY | Architecture | Branding | Interiors | Planning](https://ktgy.com/work/park-house/?ref=edwarddeaver.me)\n\nBut the idea of turning parking lots into gardens to address food insecurity is not:\n\n[Community garden to turn barren Deltassist parking lot into a ‘paradise’ - North Delta Reporter](https://www.northdeltareporter.com/news/community-garden-to-turn-barren-deltassist-parking-lot-into-a-paradise/?ref=edwarddeaver.me)\n\nAnd roof top gardens aren't either:\n\n[Cambridge Center Roof Garden photos](https://www.yelp.com/biz_photos/cambridge-center-roof-garden-cambridge?select=MT5kM2JApH_EYuUYXq0OrQ\\&ref=edwarddeaver.me)\n\n[Herbs & veggies growing here!](https://www.yelp.com/biz_photos/cambridge-center-roof-garden-cambridge?select=MT5kM2JApH_EYuUYXq0OrQ\\&ref=edwarddeaver.me)\n\n[Yelp](https://www.yelp.com/biz_photos/cambridge-center-roof-garden-cambridge?select=MT5kM2JApH_EYuUYXq0OrQ\\&ref=edwarddeaver.me)\n\nSo what if we combined the two to address growing food insecurity and housing availability?\n\nWhat if with urban farming and modern green houses you turned the parking lots into housing and food production?\n\nBasement - Rain collection tanks\n\n2nd floor - green houses tended to by the apartment dwellers (example something like [Bowery Farms](https://www.yelp.com/biz_photos/cambridge-center-roof-garden-cambridge?select=MT5kM2JApH_EYuUYXq0OrQ\\&ref=edwarddeaver.me) but scaled down - I'm not sure if you could have some type of lens/mirror setup to funnel outside light into it - the scale would be determined by electricity costs).\n\n3rd floor - apartments that are very space efficient with systems inside like [Bumblebee Spaces](https://www.bumblebeespaces.com/?ref=edwarddeaver.me).\n\nTop floor - rain collection and solar panels.\n\nI typed this imaginative future into the Firefly Image Generator by Adobe and got some very interesting results. Even when not specifying \"Solar Punk\" the images reflected the ideas.\n\nImages below generated by Adobe Firefly [https://firefly.adobe.com/](https://firefly.adobe.com/?ref=edwarddeaver.me)\n\n\u003Cdiv class=\"gallery\">\n  \u003CAstroImage src=\"/assets/blog/Firefly_Parking-lots-turned-into-vertical-gardens-in-a-dense-city._photo_12671.webp\" alt=\"Parking lot turned into green\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/Firefly_Parking-lots-turned-into-vertical-gardens-in-a-dense-city.-Solar-punk._art_49187-2.webp\" alt=\"Parking lot turned into green\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/Firefly_Parking-lots-turned-into-vertical-gardens-in-Detroit_art_12671.webp\" alt=\"Parking lot turned into green\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/Firefly_Parking-lots-turned-into-vertical-gardens-in-Detroit_art_49187.webp\" alt=\"Parking lot turned into green\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/Firefly_Parking-lots-turned-into-vertical-gardens-in-a-dense-city._photo_49187.webp\" alt=\"Parking lot turned into green\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/06.jpg\" alt=\"qurno astrojs blog theme\" width={970} height={500} />\n\u003C/div>\n\nI hope we can achieve something like this to address growing food insecurity, and housing.","src/content/blog/turn-the-parking-lots-to-gardens.mdx","e00cc5bbe7a59674","turn-the-parking-lots-to-gardens.mdx",true,"a-fascinating-discussion-dinner-with-bitternut-homestead",{"id":29,"data":31,"body":40,"filePath":41,"digest":42,"legacyId":43,"deferredRender":28},{"title":32,"description":33,"image":34,"date":35,"author":18,"categories":36,"tags":37}," A Fascinating Dinner with Bitternut Homestead","The housing market in Syracuse is broken (read the Syracuse Housing Study), like most places. The city is majority single-family land use for housing. Let's talk about the thing giving me hope. ","assets/blog/IMG_4973-2.png",["Date","2023-03-31T04:00:00.000Z"],[23],[38,39],"co-living","coop","\u003CAstroImage src=\"assets/blog/IMG_4973-2.png\" alt=\"6 people sit at a table at Bittnut Homestead\" width={2000} height={1500} caption=\"\" />\n\nDinner at Bitternut.\n\nThe housing market in Syracuse is broken (read the [Syracuse Housing Study](https://edwarddeaver.me/blog/a-fascinating-discussion-dinner-with-bitternut-homestead/Syracuse%20Housing%20Study)), like most places. The city is majority single-family land use for housing. Let's talk about the thing giving me hope. [Bitternut Cooperative Homestead](https://bitternutcooperativehomestead.wordpress.com?ref=edwarddeaver.me) is a great initiative by Frank Cetera to build a better model of housing in Syracuse. One that breeds community, affordable housing, and bettering the environment.\n\nLet's talk about hope for a better future:\n\n### Let's first define some terms:\n\n* Co-op (Cooperative): \"For practical intents and purposes, a co-op can be defined as a building that is jointly owned by a corporation made up of all its inhabitants. When you buy into a co-op, you’re not purchasing a piece of property – rather, you’re personally buying shares in a nonprofit corporation that allows you to live in the residence.\" ([RocketMortage](https://www.rocketmortgage.com/learn/what-is-co-op?ref=edwarddeaver.me)) You may have interacted with one through smaller banks/credit unions. Or if you are in NYC, maybe you'll see these listed on [https://streeteasy.com/](https://streeteasy.com/).\n* Intentional Community: You may have heard the term commune before especially in the context of 1960s/70s. They're different, \"Cohousing is a type of “intentional community,” in which people make a conscious choice to live together as a group. However, it’s not the same thing as a commune, in which a group of families jointly own a plot of land and share all their income and other resources.\" ([MoneyCrashers](https://www.moneycrashers.com/communal-living-cohousing-types-benefits-intentional-communities/?ref=edwarddeaver.me)) Bitternut is an intentional community that is co-op housing, it's not a commune.\n\n## What is Bitternut Homestead?\n\nI don't think I can do this better than quoting the [prospectus](https://drive.google.com/file/d/1f5Q9fg_f_oiblZgi4_amXQb_Jfg_WRqH/view?ref=edwarddeaver.me).\n\n\"The Bitternut Homestead is located at 717 Otisco St, in the Near Westside, near downtown Syracuse, NY; and provides products and services, including:\n\n* Shared Home - a 1,900 sq. ft. home with access to full kitchen, pantry, dining room, sitting/tv room, wood burning cast iron stove, multi-purpose room, home office space, half-bath/half-utility room, clothes washer, enclosed side-porch bicycle parking, attic and basement storage, private bedrooms, full bath on 2nd floor with walk-in shower and cast iron clawfoot bathtub, guest room.\n* Affordability - 4 Residents at current $350 a month afford a whole 3-bedroom house at $1,400, under the neighborhood average of $1,446, while offering over double the space of an average apartment of 830 Sq. Ft.\n* Edible Landscaping and Kitchen Gardening - All residents contribute 2 hours weekly to garden & landscape work. The value this returns is hard to measure but consider our garlic harvest as one example. By spending 2 hours together preparing beds and planting 100 cloves, we harvested 80 bulbs this year.\"\n\nCheck out their Instagram - if you are also vegetarian or just into urban farming check it out:\n\n\u003Ciframe class=\"instagram-media instagram-media-rendered\" id=\"instagram-embed-0\" src=\"https://www.instagram.com/p/CqWpIIyJGhu/embed/captioned/?cr=1&v=14&wp=1080&rd=https%3A%2F%2Fedwarddeaver.me&rp=%2Fpersonal-blog%2Fa-fascinating-discussion-dinner-with-bitternut-homestead#%7B%22ci%22%3A0%2C%22os%22%3A882%2C%22ls%22%3A382%2C%22le%22%3A866%7D\" allowtransparency=\"true\" allowfullscreen=\"true\" frameborder=\"0\" height=\"790\" data-instgrm-payload-id=\"instagram-media-payload-0\" scrolling=\"no\" style=\"background: white; max-width: 540px; width: calc(100% - 2px); border-radius: 3px; border: 1px solid rgb(219, 219, 219); box-shadow: none; display: block; margin: 0px 0px 12px; min-width: 326px; padding: 0px;\" />\n\nThis was the first time I had actually met Frank in real life. Somehow we were connected on LinkedIn and I'd been following Bitternut Instagram. He's a nice guy, it's very clear he's genuinely passionate about this.\n\nAlso, this was the first time I had met the other guests at the table.\n\nFrank made a great meal from veggies from their garden, a guava paste salsa from Cuba, and a wonderful gooseberry jam from the garden with the churro waffles.\n\nDavid educated me on student housing [co-ops in Ann Arbor that was started in response](https://icc.coop/?ref=edwarddeaver.me) to a lack of affordable housing. Could students do that here?\n\nCould the residents of Syracuse do that?  Well, they are and we'll get to that.\n\n### Notes:\n\n#### Other student housing groups:\n\n*     [Cornell](https://scl.cornell.edu/residential-life/housing/campus-housing/upperlevel-undergraduates/cooperative-housing?ref=edwarddeaver.me)\n  * Not fully related but Cornell has a whole website about co-ops in their general form and founding them. Very interesting stuff - [http://cooperatives.dyson.cornell.edu/](http://cooperatives.dyson.cornell.edu/?ref=edwarddeaver.me)\n* [MIT](http://lgc.mit.edu/general_info?ref=edwarddeaver.me)\n\nLindsey told me about another co-op in Syracuse, [Bread and Roses](https://www.breadandrosescollective.org/,?ref=edwarddeaver.me), ([Facebook is more up-to-date](https://www.facebook.com/breadandrosescollective/)). As well as [Syracuse Grows](https://syracusegrows.org/?ref=edwarddeaver.me), a network of community gardens in Syracuse.\n\nThere was talk about the co-op model and how the shares work. Class A vs B and the roles and responsibilities included in them. I like how the higher the investment of money you put in (Class A) you get voting rights but to retain ideological investment there is also a work requirement in it.\n\nAlso, in general, there was talk about community and foraging for wild plants in the city.\n\nCommunity is a key to their success, there is something very special happening there.\n\nAfter learning about co-ops my mind is racing with ideas of could I start one?\n\nSo I've now rented [\"Utopia Drive\" by Erik Reece](https://us.macmillan.com/books/9780374710750/utopiadrive?ref=edwarddeaver.me) to learn about the current landscape of commune-ish intentional communities and[ \"America's Communal Utopia's\" edited by Donald E. Pitzer ](https://uncpress.org/book/9780807846094/americas-communal-utopias/?ref=edwarddeaver.me)to get a historical lens on it from the library.","src/content/blog/a-fascinating-discussion-dinner-with-bitternut-homestead.mdx","e56218dd75800f65","a-fascinating-discussion-dinner-with-bitternut-homestead.mdx","making-of-bachelors-capstone-project",{"id":44,"data":46,"body":59,"filePath":60,"digest":61,"legacyId":62,"deferredRender":28},{"title":47,"description":48,"image":49,"date":50,"author":18,"categories":51,"tags":53}," Making Of: Bachelors Capstone Project","aThe creation of my Computer Science Bachelors capstone using NodeJS, Raspberry Pi, Arduino, openFrameworks, and Processing.","assets/blog/blenderleds.png",["Date","2020-01-31T05:00:00.000Z"],[52],"interactive",[54,55,56,57,58],"openFrameworks","NodeJS","RaspberryPi","Arduino","Processing","The creation of my Computer Science Bachelors capstone using NodeJS, Raspberry Pi, Arduino, openFrameworks, and Processing.\n\n\u003CAstroImage src=\"assets/blog/blenderleds.png\" alt=\"Person standing in front of a wall of leds\" width={601} height={591} />\n\nA render of my dream, a massive led grid maybe 10' x 10'. Sigh, one day.\n\n## Initial Steps:\n\nInitially, I considered using Twitter as a medium for interactivity. So I applied for developer access - but unlike the normal application if you are using this on behalf of a government you have to say so, and the form gets a lot longer. After stating what I was doing with the API, I received a follow-up email asking me to restate what I said. By that time over a week had passed and I moved the project in a different direction for interactivity. (Edit from the future: Twitter is now called X, owned by Elon Musk, and API access has gone away totally so… dodged a bullet and learned don’t rely on external platforms).\n\nThese are quotes from the initial idea document:\n\nI want interactivity as opposed to passive sensors(like a temperature sensor because you can just ask your phone what the temperature is or look outside if it’s snowing). So, let’s make a bot(twitter) that takes in photos via an @ mention then cuts them into 3×3 grids(OR higher resolution grids 8×8, 16×16) then finds the dominant color in each “quadrant” then exports that to a LED light grid, with a screen saying who sent the photo. Then a webcam on the other side of the room takes a photo of it and tweets the photo and @’s the person who sent it.\n\nThis bypasses sensors entirely and forces people to interact with technology to see a physical manifestation of digital actions. By creating physical experiences we can break down the mystification of technology and create magic. We will also have a plaque/website saying in plain English how this works.\n\nFlow:\n\n  \n\n1\\.  Tweet at the bot this image: (Author @unsplash)\n\n\u003CAstroImage src=\"assets/blog/stock-image-225x300-2.png\" alt=\"Stock image of a building looking up\" width={225} height={300} />\n\n2\\. The program will download the image then start to analyze it:\n\n\u003CAstroImage src=\"assets/blog/stockimagegrid-225x300-2.png\" alt=\"That building with a grid on it\" width={225} height={300} />\n\n3\\. Then find the dominant color of each box in the grid:\n\n\u003CAstroImage src=\"assets/blog/stockimagegridcolor-225x300-2.png\" alt=\"Grid of the image with the most dominant color in each grid\" width={225} height={300} />\n\nThen represent those colors on a grid of LEDs, and put the tweeter’s name on a screen.\n\n3.a Then use a webcam on the other side of the room to take a picture of it and tweet out the photo and @ the tweeter.\n\n### Applying to Twitter for API access\n\nTo get an API key for Twitter you need to apply, with a specific requirement that you disclose if you are a government entity. This disclosure requirement caused major delays in my application and eventually its refusal.\n\n### Major pivot to the main idea – Midterm:\n\nBy this point the project hardware solidified into using a Raspberry Pi 4 as the master device and using Arduinos as sensor interfaces over the USB. The Raspberry Pi would control Neopixel LEDs using a FadeCandy a Teensy based Neopixel control USB board (Edit from the future: unfortunately the FadeCandy platform does not exist anymore):\n\n\u003CAstroImage src=\"assets/blog/Slide5.jpg\" alt=\"Slide of the hardware\" width={960} height={540} />\n\nThe initial application that ran on the RB Pi 4 was made using [Processing](https://processing.org/) a Java-based creative coding framework. I eventually moved from this due to low FPS when reading from the serial input and writing to the FadeCandy. The Processing library for the FadeCandy works by establishing points on the canvas to sample colors from the canvas. Those colors are then sent via the Open Pixel Control library to the FadeCandy server then that sends its data to the FadeCandy.\n\n\u003CAstroImage src=\"assets/blog/Slide6-1.jpg\" alt=\"Software architecture \" width={960} height={540} />\n\nExample of distance sensor working:\n\n\u003CVimeo id=\"852516514\" />\n\nUsing ultrasonic distance sensors controlled via Arduino, receiving data into Processing program and led mapping distance.\n\nIn order to achieve my dream of a large led wall that still existed at this time I planned on moving the application to a Mac Mini and increasing the amount of FadeCandys to 3 and tripling the increasing the amount of NeoPixel strips to 24 strips. The FadeCandys can support 8 led strips. This did not happen.\n\n\u003CAstroImage src=\"assets/blog/Slide11.jpg\" alt=\"Hardware slide now using mac mini and multiple esp32 and 3 fadecandys \" width={960} height={540} />\n\n### The Midterm to Final:\n\n### Move toward openFrameworks:\n\nThis began my journey from my comfortable Java world to C++. I started to remake the program in OpenFrameworks 0.10.0 using XCode on Mac. Due to OpenFrameworks 0.10.0 not supporting the Raspberry Pi 4 I had planned on using a Mac Mini as the main hardware platform. Because this is C++ and not compiling for JVM each compilation needs to be done on the specific device you’re targeting. Eventually, OpenFrameworks 0.11.0 would be released and support the Raspberry Pi 4, which would end up being the final platform for this project.\n\nEventually, I was able to get to a point where I could control the position of boxes in openFrameworks (Mac) using 2 ultrasonic sensors as X / Y controllers.\n\n\u003CVimeo id=\"852517849\" />\n\nDuring this time I also tried to make a standby animation using time as a variable to create a smooth gradient. This resulted in \\~200 rectangles per rectangle. This was not added to the final product.\n\n\u003CVimeo id=\"852518345\" />\n\nTime based animation of rectangles mapped to LEDs using openFrameworks. \n\nOnce the interaction of the sensors and the FadeCandy was complete I tried to add more IoT features: writing sensor data to a database and a website.\n\nFor the database I chose to go with AWS DynamoDB, a NoSQL database. I chose DynamoDB because it had a very generous free tier and I wanted to get experience using AWS.\n\nFor the website I made NodeJS Express Post endpoint that used a internal SocketIO connection to send new data received from the Post request to the client. I also created a really smooth pulsating green light that updated when the data was received. I wanted to make the site to give another way for someone not able to visit the instillation.\n\n\u003CVimeo id=\"852519092\" />\n\n### Final:\n\n### System data flow:\n\nThe data in the application is mainly gathered and passed around in the OpenFrameworks update function. First update the OPC function. Then restart the stage, which is essentially wiping the canvas. Then if Serial 1 is available get the data, convert bytes to float, and set it to newx1. Then if Serial 2 is available get the data, convert bytes to float, and set it to newx2. If the timer is >60 minutes reset timer and stage. Then it will play a predefined tone based on the newx1 amount, and newx2 amount. Then, it will draw the amount of squares using the linear squares for newx1 and newx2. Next, it will reach the stage pixels and send it to OPC FadeCandy. Then it will send the sensor data to the UDP socket.\n\n\u003CAstroImage src=\"assets/blog/SystemDataFlow.jpg\" alt=\"The image illustrates a flowchart of a system data flow for an update function, detailing various operations involving OPC library functions. The flow starts with resetting OPC timer/error codes and proceeds through a series of steps, including:  Initiating the stage with OPC.beginStage() to create the stage rectangle and buffer.  Handling audio playback functions (audioPlayBackAtPoint) based on input data.  Drawing linear squares for data visualization (DrawLinearSquares).  Sending sensor data via UDPSEND to a websocket server.  Managing system timer for tasks such as resetting the stage after 60 minutes.  Completing stages with OPC.endStage() and capturing pixel data for LED strips.  Writing to channels with OPC.writeChannel() and connecting to the OPC client if not already connected.  Each step includes specific conditions like checking data amounts, converting data, and setting frequencies.\" width={960} height={540} />\n\nThe color data flow:\n\n\u003CAstroImage src=\"assets/blog/ColorDataFlow.jpg\" alt=\"The image shows a data flow diagram for controlling lighting effects using serial data input. The flow is split between left and right lights, with each side receiving data from Serial1 and Serial2 inputs. The input values are divided into four categories based on their size:  Category 1 (Input > 6000)  Category 2 (Input > 4000)  Category 3 (Input > 2000)  Category 4 (Input \u003C 500)  For each category, the system draws a specific area of rectangles, with each area consisting of 70 rectangles to produce a gradient effect. The left and right lights are controlled independently based on the corresponding inputs, with each category drawing a different number of rectangle areas for visual effects.\" width={960} height={540} />\n\nEx. Left lights:\n\nWhen a number is input it will check the amount and determine how many level of boxes will enraged. Each category will draw a “rectangle”. The lower the input amount the more rectangles will be drawn due to the distance sensors. This means the user is very close to the sensor. Each “rectangle” is made up of 70 rectangles to produce a gradient.\n\nThis shows both of the lights effect.\n\nNetwork flow (openFrameworks):\n\nHow the OpenFrameworks application works it sends a UDP message of the data to a Python UDP Socket. The Python app sends that to a NodeJS app, and to AWS DynamoDB. The NodeJS app receives that data and sends it to an internal SocketIO room that the client site connects to. Note the Github link does not have the AWS DynamoDB write code, unfortunately I believe the only version exists on the Raspberry Pi.\n\n\u003CAstroImage src=\"assets/blog/NetworkFlow.jpg\" alt=\"The image illustrates the network flow for a system involving various technologies. The flow begins with the FadeCandy Server, which communicates through a socket with OpenFrameworks. OpenFrameworks then uses a UDP socket to connect to a Python WebSocket Server and a REST Client.  On the web side, there is a connection to Socket IO Client hosted on Heroku, which communicates with a NodeJS Express / Socket IO server. This server sends HTTPS POST requests to an AWS DynamoDB database for data storage and management. The diagram shows a clear division between local (localhost) and web-based communication paths.\" width={960} height={540} />\n\nNetwork flow (NodeJS):\n\nUses Express to create a REST API (Get for the website)\n\n    Get (/): returns the website\n\n    Post (/sendsensorsata) Receives the input JSON data from the Python app – Does input validation on the request – Validates key is equal to key stored as environment variable in NodeJS – Validates sensor1 and sensor1 are not undefined, numbers. – If the request is valid sends the sensor data to the socketIO client.\n\nWiring diagram:\n\nThe wiring diagram for the installation is very simple. The power distribution for the WS2812 NeoPixel LED strips is in parallel and the data lines are run to the FadeCandy. The ESP8266s are connected to the Raspberry Pi via USB to Serial cables. The distance senor have a trigger and echo connections as well as power and ground connections.\n\n\u003CAstroImage src=\"assets/blog/FINAL-Presentation.015.jpeg\" alt=\"Here's alt text for this hardware wiring diagram: &#x22;Hardware wiring diagram showing connections between a Raspberry Pi, FadeCandy controller, ESP8266 modules, distance sensors, and WS2812 LED strips. The diagram includes a 5V 12A power supply connected to WAGO lever nuts for power distribution. Color-coded wiring shows power ground (blue), data ground (black), data positive (white), and power positive (red) connections. The Raspberry Pi connects to a FadeCandy board, which then connects to four separate WS2812 1-meter 60-LED strips. Two ESP8266 modules are shown connected to distance sensors. All components are interconnected through the WAGO terminal blocks for organized power and data distribution.\" width={960} height={540} />\n\nThe hardware wiring for the installation. the led strips are ran in parallel from a 5v 12a power supply. the data lines are connected to a fadecandy. the esp8266s are connected via usb to serial cables, and the distance sensors are connected to trigger and echo pins, and power / ground.\n\nUnderstandable definitions:\n\nThese are the definitions that were used to make the posters and make the technology used more accessible.\n\nNeoPixels: These lights are like Christmas lights. Each light can be changed to a different color.\n\nArduino: This is a “microcontroller”. It’s a device that are told to do one thing and they do it forever. They are similar to how your cell phone works but instead of being able to run multiple apps at once, they can only run 1 app.\n\nDistance Sensor: This allows the Arduino to tell how close you are. This is like motion detectors on garage lights, or automatic doors at your favorite retail store.\n\nFadeCandy: This tells the light strip which individual lights to light up and what color. It’s like a remote for the lights but controlled from the computer.\n\nRaspberry Pi: This is a very tiny computer. It works just like your laptop but is super compact. The computer runs a program that listens for the Arduino to say something – then tells the lights to light up.\n\nGithub: The main application: [https://github.com/EdwardDeaver/SyracuseInnovationLEDProject](https://github.com/EdwardDeaver/SyracuseInnovationLEDProject)","src/content/blog/making-of-bachelors-capstone-project.mdx","97d9d486982b1eb6","making-of-bachelors-capstone-project.mdx","web-scraping-instead-of-api",{"id":63,"data":65,"body":74,"filePath":75,"digest":76,"legacyId":77,"deferredRender":28},{"title":66,"description":67,"image":68,"date":69,"author":18,"categories":70,"tags":72}," Web Scraping Instead of Using the API","Scraping YouTube chat data instead of using Google's bad api. ","assets/blog/selenium_logo.webp",["Date","2020-10-30T04:00:00.000Z"],[71],"web-scraping",[73],"python","For my latest project I wanted to create a chat bot that took in color commands from both YouTube and Twitch. Making a Twitch chat bot is incredibly easy from good documentation, and bot registration to using a websocket/IRC connection for the bot instead of HTTP polling. YouTube makes it very hard to create a real time chat bot.\n\nNote: for this post I am going to be using 1≤ as real time information, and anything above that not real time nor near-real-time.\n\nThis post will be covering:\n\nWhat makes the YouTube API difficult? Creating a Selenium based webscrapper and its challenges. So what makes the YouTube API difficult? The API uses polling for live chat messages. (Even Facebook provides a stream of data for their live comment data) The usage quota. How do you get around these limitations?\n\n    The polling Polling is the process of querying an API Endpoint (Ex. mysite.com/CurrentDiamondPrice) a number of times per Y in order to get new information, but with the assumption that it will not be in realtime. This practice is fine for getting subscriber counts or a channel name, but when we deal with real time data like a chatroom that could have 100 new messages a second this fails to work.\n\nBefore we get started: YouTube will not support websockets according to u/marcchambers https\\://www\\.reddit.com/r/youtubegaming/comments/4kibiw/api\\_reading\\_live\\_chat\\_messages\\_from\\_javascript/d3gina7/\n\nYouTube is the only major streaming platform that has not implemented some type of streaming network interface. Twitch, Facebook and Mixer have (had in the case of Mixer).\n\n    The Quota. Every action taken with the YouTube API costs something, and these are charged to your quota amount (note using the API is free).\n\nThe current quota (July-2020) is 10,000 units.\n\nPost-April 2016\\:the quota was 1 Million.\n\nIn 2015\\:the quota was 50 Million.\n\nBecause in order to get new information from the API we have to poll it, we have to resort to an inefficient way of getting new information. It is entirely possible that we could hit the API and get no new information but have burned a credit.\n\nSo let’s assume you want to make a chatbot that just reads chats, so 1 call to the API costs 1 credit. Let us also assume this chat bot wants to operate in nearly real time so let’s say 1 second between calls ( for the purposes of this bot I will use 1 second as realtime, if this was a graphical application real time would be \\~16ms).\n\nSo if we were to make 1 call a second that means that we would burn through our API quota in 2.7 hours. This is longer than a majority of streams but what if you are doing a 24 hour stream? How quickly could your bot update if it has to run for 24 hours?\n\n24 hours in seconds is 86,400‬ seconds.\n\nWe have a cap of 10,000 hits.\n\nSo, 86,400‬ / 10,000 = 8.64 seconds per API call.\n\nSome of the issues arising from this are you now need to do data processing on 8 seconds worth of messages which, due to the polling nature of our call, is going to be of an unknown amount. If this were a socket we could do on-demand processing for a message. Also when we process a message that is not the last message in our list the time the color associated is on the LEDs will be cut short because we are looping through a dataset instead of waiting \\~1 second per new command. That message color could be on the lights for \\~100ms.\n\nCreating a Selenium based web scraper and it’s challenges. Ok, here is the fun part of this post.\n\nFirst we need to isolate the chat, which can be done using the pop-out window URL for YouTube chat.\n\nThen we need to get the chat messages from the chat using the CSS Selector for the class “yt-live-chat-text-message-renderer”.\n\nTraditionally, the tutorials I’ve read up to this point deal with chat data by just getting querying the chat messages and overwriting it when it’s displayed on a screen. My implementation works like a websocket would. From this we have a snapshot of the messages (remember that the chat box is still filling up at this time). So in order to make a usable scraper that will be able to handle new messages coming in we have to store length of variables and splice our chat messages from the CSS selector so:\n\nGet the chat data and tell our get chat function to return messages with a start point of 0 startData = pointChatData(0) #START POINT Data This data returned is an array of \\[len(chatData),chatData] Now store the start point we will use in the next calls. \n\nThe program needs to do this so that the subsequent loops know where to start: \n\n    startPoint = startData\\[0] \n\n    Start point is set to the length of StartData Enter a while loop that doesn’t end: \n\n        while(True): \n\n            Get the chat data and tell our get chat function to return messages with a start point of startPoint startData = pointChatData(startPoint) \n\n            \\#START POINT Data \n\n            Now store the start point we will use in the next calls. The program needs to do this so that the subsequent loops know where to start: \n\n                startPoint = startData\\[0] \n\n                \\#Start point is set to the length of StartData \n\n                Now, comes the part of dealing with a dynamic data structure you don’t control. YouTube stores the chat data in a stack and once the stack hits \\~248 data points the first messages start to be removed from the stack. \n\n                Now let’s get the lastIndexPosition of our chatset: \n\n                lastIndexPosition = len(startData\\[1])-1 \n\n                Now let’s get the last message sent in that message data set. \n\n                lastMessage = startData\\[1]\\[len(startData\\[1])-1] \n\n                Now, let’s get the position of the last message sent in the new chat message dataset and set that index position as the start point. \n\n                    startPoint = getNewestPosition(lastMessage, startData\\[1]) \n\n                    At this point the message length should be sub 248 and it will go back to A then to B till it hits above 248 and do it again.\n\nThis solution isn’t perfect by any means. All of this has the overhead performance cost of running a separate installation of Chrome web browser, but it works.","src/content/blog/web-scraping-instead-of-api.mdx","80e6bc0ef5ecf198","web-scraping-instead-of-api.mdx","mit-reality-hack-2023-blog-day-1",{"id":78,"data":80,"body":91,"filePath":92,"digest":93,"legacyId":94,"deferredRender":28},{"title":81,"description":82,"image":83,"date":84,"author":18,"categories":85,"tags":87}," MIT Reality Hack 2023 Blog Day 1","Day 0 and 1 of my time at MIT Reality Hack. That was insane, the energy amazing, the creative drive unreal, the problems challenging, the learning academic. I have not felt that for a very long time.\n","assets/blog/team_photo-1.jpg",["Date","2023-01-01T05:00:00.000Z"],[86],"hackathon",[88,89,90],"mit reality hack","xr","boston","Day 0 and 1 of my time at MIT Reality Hack. That was insane, the energy amazing, the creative drive unreal, the problems challenging, the learning academic. I have not felt that for a very long time.\n\nThat was insane, the energy amazing, the creative drive unreal, the problems challenging, the learning academic. I have not felt that for a very long time. No problem at work has ever stimulated me that much, no vibe was ever that wavey, nothing has been that hard, and I have never felt that energy before.\n\nHope I can go back. I've found what I want.\n\nI have a million ideas now, so many blog post starters in my Notes app, so many project ideas. So many racing thoughts.\n\nI want to take you on my journey of Reality Hack '23, a bit shortened and expanded.\n\n## Day 0: Travel\n\nSyracuse to Boston via Amtrak takes 9 hours. Next time I'll just fly, hope one day we get high speed rail. After arriving to a hostel and seeing my bunk mates mess underneath me I checked into a hotel that night. To get to the hotel I took a Lyft and this starts a pattern in Boston of fascinating Lyft drivers:\n\nDriver 1:  I have never had someone talk to me Securities and Crypto at such length, and about TeleAviv. This man had learned the lesson of \"not your keys, not your coins\". If you think of a crypto scam, he'd been affected by it (Mt. Gox, Celsius, to name a few). Went from Chemistry to a Finance major. Also told me all about ICOs (Initial Coin Offerings) via Telegram channels. Welcome to Boston.\n\nEventually went to sleep at 1 AM.\n\nThe end of day 0.\n\n## Day 1: First day of MIT Reality Hack 2023\n\nWake up: 6AM.\n\nThe hotel was a 30 minute walk to MIT Media Lab for check-in; cut through a park and walk the compact streets of Cambridge, love walkable cities. Cambridge is pretty walkable, on the MIT area of town there are even fully protected sidewalks with protected bike lanes. I think this might make some Central New York politicians faint at the inconvenience to cars or the idea of protecting cyclists/pedestrians (looking to move). While I've heard bad things about Boston drivers (I'm going to combine all of the surrounding areas under \"Boston\" for this \\:P ), they were generally nice, but that's been true of most places I've been to so far.\n\n\u003CAstroImage src=\"assets/blog/Screenshot-2023-01-21-at-2.08.21-PM.png\" alt=\"Street in Boston. It's beautiful with fall leaves covering the street. \" width={2000} height={961} />\n\nI arrived at MIT Media Lab at 8AM for check in and received my badge. Time to mingle with the sponsors and network.\n\n\u003CAstroImage src=\"assets/blog/IMG_3417--1-.jpeg\" alt=\"Me a bearded bald man with my MIT Reality Hack badge\" width={2133} height={1600} />\n\nPicture of me with my badge.\n\nToday besides check-in is workshop day and opening ceremony day and team formation day.\n\nArriving at the sponsor floor was when it became very real about where I was.\n\n\u003CVimeo id=\"791488171\" />\n\nIn this video we have a few of the sponsors: [Vive](https://www.vive.com/us/?ref=edwarddeaver.me), [FCAT](https://www.fcatalyst.com/overview?ref=edwarddeaver.me), [Esri](https://www.esri.com/en-us/home?ref=edwarddeaver.me), [XR Terra](https://www.xrterra.com?ref=edwarddeaver.me), [Ultraleap](https://www.ultraleap.com?ref=edwarddeaver.me), [Dolby.io](https://dolby.io?ref=edwarddeaver.me) and right at the end is [Looking Glass Factor](https://lookingglassfactory.com?ref=edwarddeaver.me)y. A full list of sponsors can be found [here](https://www.mitrealityhack.com?ref=edwarddeaver.me) (scroll down).\n\nGeneral tip: have a notebook/doc when meeting new people, state their name, information about them - maybe physical description so you can not fumble their name when you meet them again.\n\nESRI: I met Nick from ESRI and he explained their new product[ ArcGIS Maps SDK for Unity ](https://developers.arcgis.com/unity/?ref=edwarddeaver.me)which allows you to view 3D map data in Unity/Unreal. It looks good, it looks really good. growing up, playing COD (Call of Duty) I wished I could play my neighborhood, with this you could do that. Also, I've tried getting 3D GIS data before, and it's hard. The textures are locked down and the only way to get building meshes is [3dbuildings.com](https://3dbuildings.com?ref=edwarddeaver.me) or use [RenderDoc](https://www.youtube.com/watch?v=VwE2KsWUoC0\\&ref=edwarddeaver.me) and pull the values Google Earth. I'm looking forward to using this in the future.\n\n\u003CVimeo id=\"791490476\" />\n\nFCAT: Did you know Fidelity has an R\\&D lab? I didn't till then. Also I had never used a VR headset till right then. Immediately sold (bought a Quest 2 when I got back).\n\n\u003CAstroImage src=\"assets/blog/Screenshot-2023-01-21-at-2.38.23-PM.png\" alt=\"FCAT booth\" width={1600} height={1125} caption=\"FCAT - that black headset in the middle is the Quest Pro. \" />\n\nUltraLeap: Thank you to Chris for explaining to me about the OpenXR standard and how you all are extending it by adding onto the skeleton tracking. Very interesting stuff, hope I use it in the future. I don't have a photo of this booth but here is one of their demo videos:\n\n\u003CYouTube id=\"Llvh4GBpnVA\" />\n\nDolby.io: You know how Mixer had the [FTL protocol ](https://dotesports.com/streaming/news/mixers-faster-than-light-streaming-protocol-explained?ref=edwarddeaver.me)([reverse engineering blog post of it](https://hayden.fyi/posts/2020-08-03-Faster-Than-Light-protocol-engineering-notes.html?ref=edwarddeaver.me)) Faster Than Light, and if you stream on YouTube or Twitch there is a big delay (learned tuning this delay for [Control My Lights](https://edwarddeaver.me/portfolio/control-my-lights/)). What if there was a service that allowed for sub second streaming? Well there is now Dolby.io using [WebRTC](https://webrtc.org?ref=edwarddeaver.me). Incredibly stoked for this.\n\n\u003CAstroImage src=\"assets/blog/Screenshot-2023-01-21-at-3.04.39-PM.png\" alt=\"dolby.io\" width={1600} height={822} />\n\nScroll down to \"Workshop 7:  Spatial Audio Overview\" (or just Ctrl+f/CMD+f  and go right there) to see this in action in Unity.\n\nLynx: This is very cool, a high end pass through VR / AR headset for way cheaper than a [Microsoft HoloLens](https://www.microsoft.com/en-us/hololens?ref=edwarddeaver.me): [https://www.lynx-r.com/products/lynx-r1-headset.](https://www.lynx-r.com/products/lynx-r1-headset) Ran into them walking back my hotel at night. Was nice to see a friendly face at night. Why is Cambridge so dead silent at night? Was it because the colleges were out?\n\n\u003CAstroImage src=\"assets/blog/Heroshotrt_1728x.png\" alt=\"White girl with a lynx headset outside. \" width={1440} height={8772} />\n\n[HaptX](https://haptx.com?ref=edwarddeaver.me): Didn't get to try, they look wild (in a good way). Heard they use micro-fluidics to control the pressure you feel, insane. Hope I can try it out someday.\n\n\u003CVimeo id=\"762515445\" />\n\n[Born](https://www.born.net?ref=edwarddeaver.me): Nice guys, cool demo of their Forest of Resilience. Also, had great stickers!\n\n\u003CAstroImage src=\"assets/blog/Screenshot-2023-01-21-at-3.21.27-PM.png\" alt=\"The Forest of Resilience\" width={1600} height={575} />\n\n[Looking Glass Factory](https://lookingglassfactory.com/product-overview?ref=edwarddeaver.me): video doesn't portray these well, the 3D effect is real.\n\n\u003CVimeo id=\"791508317\" />\n\nMet Gabriel (new person)- in line for something. It's serendipitous how you meet people at Reality Hack.\n\nLunch happens - met more people.\n\nCheck out this clock in front of iHQ:\n\n\u003CVimeo id=\"791510535\" />\n\n### 1pm - The workshops:\n\nNote: These were the [workshops](https://mitrealityhack.notion.site/aaa49b488efe433bb1e8386b673722c3?v=7594562b1170427bb301a5b0b835b7a7\\&p=15ac6e9e84fc45109c3e752c8fb82c5b\\&pm=s\\&ref=edwarddeaver.me): You'll quickly notice cool stuff is all happening at the same time, wish it was spread over a few days but any one you picked was good\n\n#### The first workshop: Looking Glass Factory led by Bryan Chris Brown.\n\nThese are light field displays, displays a different image depending on where you are in relation to the screen. The software creates a sprite sheet-like grid, called a \"Quilt\" and I guess the internal electronics do the rest. You can even attach it straight to Unity/Unreal. Check out their docs [here](https://docs.lookingglassfactory.com?ref=edwarddeaver.me).\n\n\u003CVimeo id=\"791511068\" />\n\n#### 2nd workshop:  [Cognitive3d](https://cognitive3d.com?ref=edwarddeaver.me) workshop.\n\nI decided to rest and stay and stayed at the same room for the Cognitive3d workshop. A tool that allows you to gather geospatial analytics in the VR environment, as well as do eye tracking analytics. I can see using this for iterating over an interaction to fine tune it, especially in game design.\n\n#### 3rd workshop: Intro to the hardware hack\n\naka \"DIY Open Source Hardware\" led by Bryan from Looking Glass/ [Project Northstar](https://www.projectnorthstar.org?ref=edwarddeaver.me) and [Lucas De Bonet ](https://www.linkedin.com/in/ldebonet/?ref=edwarddeaver.me)of [LucasVR](https://www.youtube.com/c/lucasvrtech?ref=edwarddeaver.me) ([LucidVR](https://github.com/LucidVR/lucidgloves?ref=edwarddeaver.me)).\n\nMeeting Lucas was so cool after seeing him on Linus Tech Tips’ video.\n\nBoth talked at length of about community building at having an open source project that are challenging the industry. This got me pumped for it.\n\n\u003CAstroImage src=\"assets/blog/Frame-21-01-2023-05-22-00.png\" alt=\"Brian on stage\" width={1280} height={720} />\n\nThat's Bryan and Project Northstar - checkout his light up shoes.\n\nThese are the LucidVR gloves:\n\n\u003CVimeo id=\"791514792\" />\n\n#### Workshop 4: Walking Through the Hardware Hack\n\nThis was basically a \"what to expect\". How to scope, what you'll doing, what you'll get in terms of hardware. But the crown jewel of this talk was [TheSingularity](https://github.com/VRatMIT/TheSingularity-Unity?ref=edwarddeaver.me) - \"This is a software development kit for Unity that allows you to make microcontroller-based VR hardware communicate to your Unity project. Currently it supports communication via Bluetooth Serial between microcontrollers and Android-based VR headsets.\" (the README.md). This was/is a very new library made by students over winter break. Everything about your projects will be Alpha.\n\n\u003Cdiv class=\"gallery\">\n  \u003CAstroImage src=\"/assets/blog/IMG_3434.jpeg\" alt=\"Slide about how singularity works\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3438.webp\" alt=\"How hardware hack will work\" width={970} height={500} />\n\u003C/div>\n\n#### Workshop 5: Rapid Prototyping and Team Management by XR Terra.\n\nFantastic workshop on project management. Probably should have asked for the slides and studied them further. I think we did good project management but there  were areas to improve on.\n\n\u003Cdiv class=\"gallery\">\n  \u003CAstroImage src=\"/assets/blog/IMG_3439.webp\" alt=\"Project Management\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3440.webp\" alt=\"Project Management\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3441.webp\" alt=\"Project Management\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3442.webp\" alt=\"Project Management\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3443.webp\" alt=\"Project Management\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3444.webp\" alt=\"Project Management\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3445.webp\" alt=\"Project Management\" width={970} height={500} />\n\u003C/div>\n\n#### Workshop 6: Spatial Audio Advanced Techniques  by [Shirley Spikes](https://www.linkedin.com/in/shirly-spikes-725a413b/?ref=edwarddeaver.me) of [Magic Leap](https://www.magicleap.com/en-us/?ref=edwarddeaver.me).\n\n\"Advanced techniques and ideas to think about when designing audio environments and interactions for AR and VR technology.\" Met Shirley IRL for the first time and got a lesson in spatial audio techniques: like diegetic/non-diegetic sounds, how do you mix audio in Unity? how do you mix that audio to give the sense of a sound is facing away from you? what software can help give spatial audio ([Steam Audio](https://valvesoftware.github.io/steam-audio/?ref=edwarddeaver.me), [Google Resonance Audio](https://resonance-audio.github.io/resonance-audio/?ref=edwarddeaver.me))?\n\n\u003CAstroImage src=\"assets/blog/IMG_3446.jpeg\" alt=\"Steam Audio logo\" width={1600} height={1200} caption=\"  Steam Audio vs Google Resonance Audio and Magic Leap. \" />\n\n#### Workshop 7: Spatial Audio Overview - Dolby.io by[ Dan Zeitman.](https://www.linkedin.com/in/dzeitman/?ref=edwarddeaver.me)\n\nI went through in this using Dolby.io to stream and receive video in Unity. This was a hands on project. Dolby.io is pretty sweet, I could also see using this for a data transfer api, if you encode your data as RGB pixel values then unpack that data on the receiving end you could pump data through it, kinda like using RGBA values in shaders to accelerate your computations by storing your data in the GPU.\n\n\u003CVimeo id=\"791334982\" />\n\nAlso, this is where I met Yash because we have the same laptop skin and were also doing the hardware track.\n\n#### Break: Dinner / Opening Ceremonies.\n\nIt's about 7PM at this point. Dinner is where I met [Logan](https://www.linkedin.com/in/lmaxsmith/?ref=edwarddeaver.me) and [Lewis](https://www.linkedin.com/in/lewisgardner/?ref=edwarddeaver.me). Opening Ceremonies were pushed back to 7:30pm ish.\n\n[Dr. Joseph A. Paradiso](https://www.media.mit.edu/people/joep/overview/?ref=edwarddeaver.me) (check out his synths \\[[link1](https://news.mit.edu/2012/video-paradiso-synthesizer-0314?ref=edwarddeaver.me), [link2](https://synth.media.mit.edu?ref=edwarddeaver.me)]) of the [Responsive Environments Lab](https://www.media.mit.edu/groups/responsive-environments/overview/?ref=edwarddeaver.me) talked about cross-reality and how our world has changed. Loved his ideas on cross-reality and putting into context how far technology has come. Similar to his grandparents (though no horse and buggy) my grandmother would take a wagon and walk to get ice for the \"fridge\" and would re-hydrate salted fish in the tub. She grew up poor in the 1920s and into the depression. She died a few years ago and to think about how the world changed in 90 years is a bit humbling.\n\n\u003Cdiv class=\"gallery\">\n  \u003CAstroImage src=\"/assets/blog/IMG_3459.webp\" alt=\"[Dr. Joseph A. Paradiso slides\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3460.webp\" alt=\"[Dr. Joseph A. Paradiso slides\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3461.webp\" alt=\"[Dr. Joseph A. Paradiso slides\" width={970} height={500} />\n\u003C/div>\n\n#### Team Formation\n\nCross reality and reflections on innovations over time. \n\nI would love to discuss with him \"Cross Reality\" and re-defining \"extended reality\" in a more human context with him. I have a bit of post written on it, it was awesome to hear someone else's take on it and give me a new term to look into.\n\nThen 2 graduate students from the ([Ferrous S Ward](https://ilp.mit.edu/node/49294?ref=edwarddeaver.me) and[ Don Derek Haddad](https://www.media.mit.edu/people/ddh/overview/?ref=edwarddeaver.me)) presented their work on VR and robotics, specifically in relation to moon exploration. As it would turn out, Don has an absolutely sick modular synth setup and played absolute bangers.\n\n\u003CAstroImage src=\"assets/blog/IMG_3465.jpeg\" alt=\"People with CRT TVs on their head in a sunflower field\" width={1600} height={1200} />\n\nSunflower field with people in suits. The people are wearing CRT monitors. \n\nI know I said the sponsor part was when it was made real to me about where reality set in. Out of the 800ish people who applied 351 were accepted, and I was one of them :).\n\n\u003CAstroImage src=\"assets/blog/acceptanceslide.jpg\" alt=\"  351 Participants, 56 Judges, 56 Mentors. Acceptance rate 66.4%.  And there were people from all over the world:\" width={1600} height={1200} caption=\"  351 Participants, 56 Judges, 56 Mentors. Acceptance rate 66.4%.  And there were people from all over the world:\" />\n\nAnd there were people from all over the world:\n\n\u003CAstroImage src=\"assets/blog/IMG_3456.jpeg\" alt=\"  Who applied to this year's hack? 858 people from around the world! TOP CITIES New York / Brooklyn v. Boston / Cambridge --> close tie! Los Angeles Atlanta Anne Arbor Berkeley Providence Pittsburgh Toronto San Francisco Seattle Waterloo Bangalore Pune Count of nationality Turkish 1.3% Pakistan 0.1% Japanese 0.4% African American 0.7% American 30.6% Indian 14.0% German 1.2% Chinese 12.6% Asian 1.6% British 07% Canadian 4.0% China 3.7%\" width={1600} height={1200} caption=\"  Who applied to this year's hack? 858 people from around the world! TOP CITIES New York / Brooklyn v. Boston / Cambridge --> close tie! Los Angeles Atlanta Anne Arbor Berkeley Providence Pittsburgh Toronto San Francisco Seattle Waterloo Bangalore Pune Count of nationality Turkish 1.3% Pakistan 0.1% Japanese 0.4% African American 0.7% American 30.6% Indian 14.0% German 1.2% Chinese 12.6% Asian 1.6% British 07% Canadian 4.0% China 3.7%\" />\n\n###\n\n### Time to make teams:\n\nThe hardware track had amazing people in it, and there definitely was a common vibe (good vibe). I don't know how to place it.\n\nTeam making was chaotic but good. Eventually once you have your team and you link up with people you have met that day and exchange discords it is time to go back to your hotel and get up early the next day. This is where I would meet Rui Jie, Jacob, Dana and Whitt.\n\n\u003CVimeo id=\"791522196\" />","src/content/blog/mit-reality-hack-2023-blog-day-1.mdx","c3dde430d5249826","mit-reality-hack-2023-blog-day-1.mdx","mit-reality-hack-2023-blog-creating-amadeus",{"id":95,"data":97,"body":105,"filePath":106,"digest":107,"legacyId":108,"deferredRender":28},{"title":98,"description":99,"image":100,"date":101,"author":18,"categories":102,"tags":103}," MIT Reality Hack 2023 Blog: Creating Amadeus (Days 2-4.5)","MIT Reality Hack days 2, 3, and 4.5: the project hacking days. This dives deep into software engineering, project management, Csound and friends.","assets/blog/IMG_3497--1-.png",["Date","2023-01-31T05:00:00.000Z"],[86],[104,89,90],"mit-reality-hack","The team on Friday. (Left to right: Me (Edward), Jacob, Whitt (in the headset), Yash, Dana.)\n\nThis is a part of a series of blogs detailing my experience at MIT Reality Hack 2023. My team’s project can be found here, [https://edwarddeaver.me/portfolio/mit-reality-hack-2023/](https://edwarddeaver.me/portfolio/mit-reality-hack-2023/). This is a continuation of the Day 0 and 1 blog post. If you haven’t read that yet, I would encourage you to do so: [https://edwarddeaver.me/blog/mit-reality-hack-2023-blog-day-1/](https://edwarddeaver.me/blog/mit-reality-hack-2023-blog-day-1/).\n\nAt the moment I’m writing it has been about 1 week 2 weeks  since the event and my memory is a little hazy. Lack of sleep and high stress (good stress in this case) take a bit more of a toll than they did when I was doing hackathons in high school, (shout out Hack Upstate). I’m using chats and photo dates as a guide for myself. This one will be a bit more technical and diving deep into certain topics.\n\n***\n\n## Friday, January 13th - Day 2\n\nDay 2 started bright and early at… 9 AM. We were all collectively dead from the night before.\n\nWe got breakfast and started project managing what we were going to do, and breaking up tasks.\n\n\u003CVimeo id=\"793695641\" />\n\nThis was 10AM :)\n\nInitially, we discussed creating a [hand-based controller similar to Imogen Heap’s](https://mimugloves.com?ref=edwarddeaver.me), see this video by Reverb to learn more about them:\n\n\u003CYouTube id=\"2jR2yi5XPqY\" />\n\nTo get started we split tasks up so we could all work asynchronously and come back together with our parts. Whitt would go for the [Singularity](https://github.com/VRatMIT/TheSingularity-Unity?ref=edwarddeaver.me) integration because he had the most Unity experience (this would have us meeting Lucas and [Aubrey](https://aubreysimonson.gitlab.io/page/?ref=edwarddeaver.me) - both mentors). Dana and I started testing our sensors and getting the code working for them. Yash was working on the mockup for the controller, and Jacob was working on Bluetooth on the ESP32 side. Dividing and conquering was interesting as a lot of us had overlapping skill sets.\n\nHere is a video discussing the idea:\n\n\u003CVimeo id=\"793696987\" />\n\nDiscussing the idea.\n\nMock up of the tactile glove:\n\n\u003CAstroImage src=\"assets/blog/IMG_3562.png\" alt=\"  Mock-up of the glove.\" width={1200} height={1600} caption=\"  Mock-up of the glove.\" />\n\nI started with getting our ultrasonic distance sensor working with the ESP32, this took longer than predicted. luckily we had the ones with timing crystals on them, never get the ones without them your data will be very messy (learned on my [Bachelors capstone project](https://edwarddeaver.me/portfolio/computer-science-bachelors-capstone/)). For next time, and for planning a project with extreme time constraints, I would give X number of minutes per task, and if they don’t get completed in that time frame cut them. This would work well for non-core parts of a project like which sensors you get working because your physical design can be adapted to the hardware. This would keep a high degree of stress at the beginning but could relieve some at the end because your vision gets automatically scoped.\n\n\u003CAstroImage src=\"assets/blog/pmt-gantt-chart.png\" alt=\"  Basically a Gantt chart and a stop watch. (credit: https://www.zoho.com/projects/project-management-tools.html) \" width={1124} height={599} caption=\"  Basically a Gantt chart and a stop watch. (credit: https://www.zoho.com/projects/project-management-tools.html) \" />\n\nEarly on this day I set up our [GitHub repository](https://github.com/EdwardDeaver/MITRealityHack2023?ref=edwarddeaver.me) and used branching to maintain our separate code bases. This normally works well, except that there were game engine files in it. The process for adding game files is very manual and requires an extensive .gitignore that Whitt provided. For game design, [Plastic SCM](https://www.plasticscm.com?ref=edwarddeaver.me) or [Perforce](https://www.perforce.com?ref=edwarddeaver.me) Version Control Systems are typically used. None of us had a license for those, so Git it was.\n\n\u003CAstroImage src=\"assets/blog/Screenshot-2023-01-28-at-9.50.07-PM.png\" alt=\"  Our GitHub branches\" width={1600} height={393} caption=\"  Our GitHub branches\" />\n\n### Software-defined vs firmware-defined data:\n\nBefore diving into this let’s define those terms. Software in this instance will be any program running on our computers or Quest 2 device, generally running at the application layer ([OSI model layer 7](https://en.wikipedia.org/wiki/OSI_model?ref=edwarddeaver.me)).  Firmware is the code running on the ESP32 allowing it to interface with the hardware (OSI model layer 1 - Physical Layer).\n\n\u003CAstroImage src=\"assets/blog/computer-network-osi-model-layers.png\" alt=\"  OSI Model (Credit: https://www.geeksforgeeks.org/layers-of-osi-model/)\" height={781} width={638} caption=\"  OSI Model (Credit: https://www.geeksforgeeks.org/layers-of-osi-model/)\" />\n\nWhen getting sensors to work on the ESP32 or any microcontroller for that matter you’ll have digital vs analog inputs. The ESP32 has 18 Analog to Digital Converter pins with a 12-bit resolution, so a minimum of 0 to a maximum of 4096 (if you were using an Arduino Uno that max would be 1024 because it is an 8-bit device). So this means our potentiometer values would be in that 0-4096 range. Our ultrasonic would not be, it works by counting digital pulses. Our buttons would also not be as they operate as either high or low, 1 or 0. You have a choice in dealing with this data getting it raw and offloading it, doing some clean up and sending it, or producing finalized analysis on it ( lots if else statements to send discrete commands). These approaches all have trade offs: speed vs clean data, clean data vs flexibility of interpretation of that data, speed vs system responsiveness (remember you only have 2 cores - even with an [RTOS](https://en.wikipedia.org/wiki/Real-time_operating_system?ref=edwarddeaver.me) you'll have trade offs).\n\nInitially, we discussed the actual values that the ESP32 would send out to address this. You can do processing on the device itself but then you eventually can run into constraints of processing speed and we are working with a system that needs to be responsive as possible so the hardware should be dumb and just offload data quickly. I pushed for going for a software-defined route for our data instead of interpreting it on the ESP32 to give us more flexibility. This can be compared to [MIDI](https://en.wikipedia.org/wiki/MIDI?ref=edwarddeaver.me) controllers controlling a software synth. The controller is just sending its raw values over predefined protocol standard and then you are left to deal with it in software.\n\n\u003CAstroImage src=\"assets/blog/doc-esp32-pinout-reference-wroom-devkit.png\" alt=\"  ESP32-WROOM-32 pinout.\" width={1384} height={675} caption=\"  ESP32-WROOM-32 pinout.\" />\n\nLet’s pop out of hardware land and jump to Bluetooth land in Unity:\n\n### Bluetooth:\n\nThis is where we would come to meet Aubrey and Lucas (I'm trying to link new people in the story as we go along but I might forget).  I wasn't very involved with getting Singularity up and running, I just knew it took a long time. According to Whitt, \"So with the singularity it was supposed to be pretty simple and the delay was from not being able to see why it wasn't working. It was a bit of blind testing but we ended getting it to work by ignoring their instructions on making a custom script with their library and just modified a script they supplied on a prefab that came with the package\"\n\n\u003CVimeo id=\"793547344\" />\n\nAn ESP32 was sending Bluetooth data, and Whitt and Lucas were debugging it. 7 pm Friday night.\n\nJacob and I (briefly) worked on finding a data format, and settled on [Arduino JSON](https://arduinojson.org?ref=edwarddeaver.me). A library that allows you to easily create JSON formatted data. I was able to point out a nested for loop would cause a poor performance, which I was pretty proud I was able to remember[ Big O](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-o-notation?ref=edwarddeaver.me).\n\nA test ESP32 would be kept around to send to send data to the Quest. Also, Bluetooth interference would start to become an issue.\n\n###\n\n### Sound Synthesis (Csound):\n\nHow do you make noise in Unity? I didn't know so I asked \"Can Unity generate audio or would a tool like [PureData](https://puredata.info?ref=edwarddeaver.me)/[MaxMSP](https://cycling74.com/products/max?ref=edwarddeaver.me)/[VCV Rack](https://vcvrack.com?ref=edwarddeaver.me) be better?\" to the Reality Hack Discord. Thank you to [Russell](https://www.linkedin.com/in/russellsng/?ref=edwarddeaver.me), [Antonia](https://www.linkedin.com/in/antonia-forster/?ref=edwarddeaver.me), Aubrey and [Yiqi](https://www.linkedin.com/in/vanyiqi/?ref=edwarddeaver.me) for helping me out. Big ups to Aubrey for suggesting [Csound](https://csound.com?ref=edwarddeaver.me) and [Csound for Unity ](https://github.com/rorywalsh/CsoundUnity?ref=edwarddeaver.me)([here are some sketches of it](https://github.com/CsoundUnityBerklee/CsoundUnitySketches?ref=edwarddeaver.me)).\n\nCsound is so interesting. It's a real time synthesis library that was made at MIT Media Lab in the 1980s by [Barry Vercoe](https://en.wikipedia.org/wiki/Barry_Vercoe?ref=edwarddeaver.me). The history of Csound is fascinating and how it flowed into other tools like PureData and MaxMSP. If you have a few minutes dive into the [Wikipedia page for it](https://en.wikipedia.org/wiki/Csound?ref=edwarddeaver.me).\n\nThis is what a [Theremin](https://www.youtube.com/watch?v=LYSGTkNtazo\\&ref=edwarddeaver.me) instrument is in Csound:\n\n```cpp\n\u003CCabbage> bounds(0, 0, 0, 0)\nform caption(\"Theremin\") size(700, 300), guiMode(\"queue\"), pluginId(\"thm1\")\nrslider    bounds(  0, 40, 80, 80), valueTextBox(1), textBox(1), text(\"Att.\"), channel(\"Attack\"),  range(0,  5, 0.15)\nrslider    bounds(100, 40, 80, 80), valueTextBox(1), textBox(1), text(\"Gain\"), channel(\"Gain\"),  range(0,  1, 0.65)\nrslider    bounds(200, 40, 80, 80), valueTextBox(1), textBox(1), text(\"Glide\"), channel(\"Glide\"),  range(0,  1, 0.15)\nrslider    bounds(300, 40, 80, 80), valueTextBox(1), textBox(1), text(\"Lfo Freq\"), channel(\"Lfo\"),  range(0,  100, 7)\nrslider    bounds(300, 40, 80, 80), valueTextBox(1), textBox(1), text(\"Lfo Amp\"), channel(\"LfoAmp\"),  range(0,  100, 8.5)\nrslider    bounds(400, 40, 80, 80), valueTextBox(1), textBox(1), text(\"Filter Freq\"), channel(\"FiltFreq\"),  range(100,  5000, 1000)\nrslider    bounds(500, 40, 80, 80), valueTextBox(1), textBox(1), text(\"Filter Res\"), channel(\"FiltRes\"),  range(0,  0.95, .066)\nrslider    bounds(600, 40, 80, 80), valueTextBox(1), textBox(1), text(\"Table\"), channel(\"Table\"),  range(0,  3.99, 1.5)\n\u003C/Cabbage>\n\u003CCsoundSynthesizer>\n\u003CCsOptions>\n-n -d \n\u003C/CsOptions>\n\u003CCsInstruments>\nsr = 48000\nksmps = 64\nnchnls = 2\n0dbfs = 1\n\nchn_k \"Frequency\", 1\nchn_k \"Amplitude\", 1 \n\n;courtesy Iain McCurdy\nopcode    lineto2,k,kk\n kinput,ktime    xin\n ktrig    changed    kinput,ktime    ; reset trigger\n if ktrig==1 then                    ; if new note has been received or if portamento time has been changed...\n  reinit RESTART\n endif\n RESTART:                            ; restart 'linseg' envelope\n if i(ktime)==0 then                 ; 'linseg' fails if duration is zero...\n  koutput    =    i(kinput)          ; ...in which case output simply equals input\n else\n  koutput    linseg    i(koutput),i(ktime),i(kinput)    ; linseg envelope from old value to new value\n endif\n rireturn\n         xout    koutput\nendop\n\n\ninstr 1\n\nkat chnget \"Attack\"\nkport chnget \"Glide\"\nkfreq chnget \"Frequency\"\nkamp chnget \"Amplitude\"\nklfo chnget \"Lfo\"\nklfoamp chnget \"LfoAmp\"\nkfiltf chnget \"FiltFreq\"\nkfiltres chnget \"FiltRes\"\nkgain chnget \"Gain\"\nktable chnget \"Table\"\nkPortTime linseg  0, 0.001, 1\nkEnvTime linseg 0, 0.001, 1\n\nkcps     lineto2    kfreq, kPortTime * kport    \nkenv     lineto2    kamp, kEnvTime * kat \nkff      lineto2    kfiltf, 0.01\nkfr      lineto2    kfiltres, 0.01\n\nalfo lfo klfoamp, klfo, 0\nftmorf ktable, 99, 100\naosc oscili kenv, kcps + alfo, 100\naout moogladder aosc, kff, kfr\n\n; left right output\nouts aout*kgain, aout*kgain\nendin\n\n\u003C/CsInstruments>\n\u003CCsScore>\n;causes Csound to run for about 7000 years...\nf0 3600\nf1 0 16384 10 1                                          ; Sine\nf2 0 16384 10 1 0.5 0.3 0.25 0.2 0.167 0.14 0.125 .111   ; Sawtooth\nf3 0 16384 10 1 0   0.3 0    0.2 0     0.14 0     .111   ; Square\nf4 0 16384 10 1 1   1   1    0.7 0.5   0.3  0.1          ; Pulse\nf5 0 16384 10 1 0.3 0.05 0.1 0.01                        ; Custom\nf99  0 5 -2 1 2 3 4 5                                    ; the table that contains the numbers of tables used by ftmorf\nf100 0 16384 10 1                                        ; the table that will be written by ftmorf\ni1 0 3600\n\u003C/CsScore>\n\u003C/CsoundSynthesizer>\n```\n\nIf you are interested in this stuff but also want to go lower check out the [Electro Smith Daisy platform](https://www.electro-smith.com/daisy/daisy?ref=edwarddeaver.me). From talking to their community on Discord a lot of the Csound ideas are implemented into the core library.\n\nThis is a demo of me controlling the synth in C#:\n\n\u003CVimeo id=\"793692233\" />\n\nVideo of theremin in Unity.\n\nCode from above:\n\n```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\nnamespace Csound.TableMorph.Theremin\n{\n\n \n    [RequireComponent(typeof(CsoundUnity))]\npublic class theraminSynth : MonoBehaviour    {\n       public  float frequencyField = 60.0F;\n       public  float amplitudeField = 1.0F;\n        public  float gainField = 1.0F;\n        public  float lfoField = 100F;\n        public  float tableField = 3.99F;\n\n\n\n        [SerializeField] Vector2 _freqRange = new Vector2(700, 800);\n        CsoundUnity _csound;\n\n\n\n        IEnumerator Start()\n        {\n            _csound = GetComponent\u003CCsoundUnity>();\n            while (!_csound.IsInitialized)\n                yield return null;\n\n            _csound.SetChannel(\"Frequency\", frequencyField);\n            _csound.SetChannel(\"Amplitude\", amplitudeField);\n            _csound.SetChannel(\"Gain\", gainField);\n            _csound.SetChannel(\"Lfo\", lfoField);\n            _csound.SetChannel(\"Table\", tableField);\n\n        }\n\n        void Update()\n        {\n            if (!_csound.IsInitialized)\n                return;\n\n            _csound.SetChannel(\"Frequency\", frequencyField);\n            _csound.SetChannel(\"Amplitude\", amplitudeField);\n            _csound.SetChannel(\"Gain\", gainField);\n            _csound.SetChannel(\"Lfo\", lfoField);\n            _csound.SetChannel(\"Table\", tableField);\n\n\n\n        }\n\n\n        // When pressed in JSON call this and play (button 0 pressed)\n        // playNote(1.0)\n        // Perfect fifth\n\n        // amplitudeField should be 0 - 1f) \n        public void setAmplitude(float Amplitude){\n            _csound.SetChannel(\"Amplitude\", amplitudeField);\n        }  \n        public void setFrequency(float Frequency){\n            _csound.SetChannel(\"Frequency\", Frequency);\n        }     \n        // position - button position on  fret board 0 at top \n\n        public void playNote(float position){\n            _csound.SetChannel(\"Frequency\", frequencyField * (position * 1.5));\n        }\n        public void changeLFO(float lfoVariable){\n            _csound.SetChannel(\"Lfo\", lfoVariable);\n        }\n    }\n}\n```\n\n## People:\n\nDuring the day we met Jim (mentor) who loved our idea and gave us math lessons on music theory. It was awesome! Stoked off that. He talked to us about circuit bending, guess what he brought on Saturday? The Guitar Hero controller also a [Bop It ](https://en.wikipedia.org/wiki/Bop_It?ref=edwarddeaver.me)and other toys.\n\nAlso talked to the other teams around us like [Hong](https://www.linkedin.com/in/studiohuahong/?ref=edwarddeaver.me) and others (these are the parts that have blurred together).\n\n\u003CAstroImage src=\"assets/blog/IMG_3497--1-.png\" alt=\"Left to right: Me (Edward), Jacob, Whitt (in the headset), Yash, Dana.\" width={1500} height={1125} caption=\"Left to right: Me (Edward), Jacob, Whitt (in the headset), Yash, Dana.\" />\n\n### Night time:\n\nThere was a nice social.\n\n\u003CVimeo id=\"793694481\" />\n\nWalking back the hotel I saw this sticker that said Lorem Ipsum (trust me, that's what this blurry photo says). This is the nerdiest sticker I've ever seen.\n\n\u003CAstroImage src=\"assets/blog/IMG_3509--3-.png\" alt=\"  Lorum ipsum sticker\" width={865} height={1153} caption=\"  Lorum ipsum sticker\" />\n\n## Saturday, January 14th - Day 3\n\nWow, that was just Friday, I got tired even writing that.\n\nGood morning...On the walk through the park I saw this. I don't know what sport this is but I've never seen one of these in a park before. It kinda looks like the sport featured in The Road to El Dorado (2000) [pok-ta-pok](https://naatikmexico.org/blog/pok-ta-pok-the-maya-ball-game?ref=edwarddeaver.me). Someone mentioned in the Discord it might be Quidditch ... maybe ¯\\\\\\_(ツ)\\_/¯ . Even in the [Cambridge PDF of parks ](https://www.cambridgema.gov/-/media/Files/CDD/ParksandOpenSpace/parksguide/ParksGuide_FINAL_20200717.pdf?ref=edwarddeaver.me)it's not mentioned. Is it like a very space efficient basketball hoop? Why is there a [key](https://en.wikipedia.org/wiki/Key_\\(basketball\\)?ref=edwarddeaver.me) on the ground behind it?\n\n\u003CAstroImage src=\"assets/blog/IMG_3511.jpeg\" alt=\"Donut shaped metal circle on metal pole.\" caption=\"Donut shaped metal circle on metal pole.\" width={1500} height={1125} />\n\nCheck out this demolition:\n\n\u003CVimeo id=\"793720083\" />\n\nToday Jim came with the toys and in his Santa bag of gifts was a Guitar Hero controller. Upon seeing it we scrapped the glove idea and went into re-using the the controller, and participated in the [Circular Economy.](https://ellenmacarthurfoundation.org/topics/circular-economy-introduction/overview?ref=edwarddeaver.me)\n\n\u003CAstroImage src=\"assets/blog/IMG_3530.jpeg\" width={1600} height={2133} alt=\"  Internals of a guitar hero controller. Much simpler than modern electronics. \" caption=\"  Internals of a guitar hero controller. Much simpler than modern electronics. \" />\n\nInternals of a guitar hero controller. Much simpler than modern electronics.\n\nDana continued on working on sensors, Jacob worked on getting Unity parsing the ESP32 content, Whitt worked on the VFX graph for particles, and I worked on Csound more.\n\nAt some point we went for a walk:\n\n\u003Cdiv class=\"gallery\">\n  \u003CAstroImage src=\"/assets/blog/IMG_3518.jpeg\" alt=\"MIT and Boston\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3519.jpeg\" alt=\"MIT and Boston\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3520.jpeg\" alt=\"MIT and Boston\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3521.jpeg\" alt=\"MIT and Boston\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3522.jpeg\" alt=\"MIT and Boston\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3523.jpeg\" alt=\"MIT and Boston\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3524.jpeg\" alt=\"MIT and Boston\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_3527.jpeg\" alt=\"MIT and Boston\" width={970} height={500} />\n\u003C/div>\n\nYou can see Boston across the river, MIT, MIT and then me.\n\nThank you to Aubrey for the MIT history: this signature in lights, \"The End of Signature, Agnieszka Kurant used artificial intelligence to create two different collective signatures realized as monumental light sculptures adorning the facades of two new buildings in Kendall Square\" [(learn more](https://listart.mit.edu/art-artists/end-signature-2021-2022?ref=edwarddeaver.me)):\n\n\u003CAstroImage src=\"assets/blog/IMG_3528-1.jpeg\" width={1600} height={1200} alt=\"  Photo of the signature ^ \" caption=\"  Photo of the signature ^ \" />\n\nThe afternoon turned into [integration hell,](https://innoroo.com/blog/2018/03/19/integration-hell-glossary/?ref=edwarddeaver.me) trying to merge my project into Whitt's, and Jacob's into Whitt's. Also, Yash and Dana had the task of switching from hand glove hardware and sensors to repurposing the guitar.\n\nCsound note: If you don't hear audio in the headset, make sure you are compiling for ARM64/ARM7 not just ARM7, that was an hour of panic wondering if Csound was compatible.\n\nI want to pause here and say without our mentors and friends we could not have completed this project: Thank you to Jim Sussino (Mentor), Riley Simone, Greg (Mentor, XR Terra), Aubrey Simonson (Mentor), Lucas (Mentor) and Chris Smoak. Seriously, without their help we couldn't have done it.\n\nThis is a short video showing Dana and Jacob talking about JSON in Unity:\n\n\u003CVimeo id=\"793723809\" />\n\nThis is a short video showing Dana and Jacob talking about JSON over Bluetooth in Unity.\n\nIntegrating my code with Whitt's:\n\n\u003CAstroImage src=\"assets/blog/IMG_3535.jpeg\" alt=\"Edward and Whitt integrating Csound code. \" width={1600} height={1200} caption=\"  Edward and Whitt integrating Csound code. \" />\n\n## Saturday, January 14th - Day 4:\n\nT-12 hours till judging. Feeling like a train on fire (thanks to stable diffusion for making that).\n\n\u003CAstroImage src=\"assets/blog/trainonfirestablediffusion.jpeg\" width={512} height={512} alt=\"Burning train car going fast on the tracks. \" caption=\"Burning train car going fast on the tracks. \" />\n\nWore the MIT Reality Hack shirt for the final day / judging.\n\n\u003Cdiv class=\"gallery\">\n  \u003CAstroImage src=\"/assets/blog/cropped-selfie.webp\" alt=\"me\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/cropped-back-selfie.webp\" alt=\"back\" width={970} height={500} />\n\u003C/div>\n\nToday was a race against the clock.\n\nWhitt would eventually hardcode the device name into it to bypass the UI breaking.\n\nWhen my part was done, I started work on our DevPost and compiling video for the video. Dana wrote the voice over, and I recorded it and edited it. That was fun to crank out a video in a hour.\n\n\u003CYouTube id=\"es4Zc46btho\" />\n\nIt working:\n\n\u003CYouTube id=\"EnQNOCDVHaA\" />\n\nFinal thoughts: my team was simply amazing.\n\nThe next post will be the last in this series talking about the judging process and the public expo (lots of photos for both).\n\n\u003CAstroImage src=\"assets/blog/IMG_3553--1-.jpeg\" alt=\"  Check out this bunny Dana made from duct tape!\" width={1600} height={1200} caption=\"  Check out this bunny Dana made from duct tape!\" />","src/content/blog/mit-reality-hack-2023-blog-creating-amadeus.mdx","64ada5392a6d7c74","mit-reality-hack-2023-blog-creating-amadeus.mdx","initial-research-electromechanical-sound",{"id":109,"data":111,"body":121,"filePath":122,"digest":123,"legacyId":124,"deferredRender":28},{"title":112,"description":113,"image":114,"date":115,"author":18,"categories":116,"tags":118}," Initial Research: Electromechanical Sound","For my booth at Syracuse Maker Faire I'm building a robot (read \"motorized\" - robotic just sounds way better) kick drum and a step sequencer. This document is me sharing my initial research into these areas. I'm also going to try some new-to-me features of Ghost CMS in this post.\n","assets/blog/52d44bcece395f7b1a8b4567-1.png",["Date","2023-03-01T00:00:00.000Z"],[117],"electromechanical",[119,117,120],"music","creative-tech","For my booth at Syracuse Maker Faire I'm building a robot (read \"motorized\" - robotic just sounds way better) kick drum and a step sequencer. This document is me sharing my initial research into these areas. I'm also going to try some new-to-me features of Ghost CMS in this post.\n\n\u003CAstroImage src=\"assets/blog/52d44bcece395f7b1a8b4567-1.png\" alt=\"Nick Poole's (Sparkfun) Arduino based sequencer. \" width={1000} height={662} />\n\n[Nick Poole's (Sparkfun) Arduino based sequencer. ](https://learn.sparkfun.com/tutorials/build-an-auduino-step-sequencer/all)\n\nFor my booth at [Syracuse Maker Faire ](https://syracuse.makerfaire.com?ref=edwarddeaver.me)I'm building a robot (read \"motorized\" - robot just sounds way better) kick drum and a step sequencer. This document is me sharing my initial research into these areas. I'm also going to try some new-to-me features of Ghost CMS in this post.\n\n#### Why make a robot?\n\n#### tldr: it's cool.\n\nFirst, let's talk about why I even want to make a worse drum machine aka a robot drum. I like the idea of electro-mechanical things, and have been thinking about the properties we inherit when we take something digital \\\u003C-> physical. I can't help but feel something is lost when your interface is purely software (ex. using a DAW with mouse and keyboard vs adding a MIDI keyboard). Initially I wanted a drum machine that you couldn't touch to play maybe making some deeper point on interactivity but that will be shelved for now (like \"you can see but can't touch\"). Also, I had considered not doing a drum but chimes, but after tapping on tubes at Home Depot with a mallet, I don't have the space for nice sounding chimes (read how to make different scales here). Also, being motorized puts on a spectacle that just a software drum can't do.\n\n\u003CCallout title=\"Being motorized puts on a spectacle that just a software drum can't do.\" theme=\"success\" />\n\nElectro-mechanical music examples:\n\nBeep Boop Click Clack\n\nHere is a collection of electro mechanical music projects that excite me and are an inspiration for this.\n\n### &#x20;1. The elephant in the room: Wintergaten's marble machine. This is mechanical but it's my blog, so...\n\n\u003CYouTube id=\"IvUU8joBb1Q\" />\n\nPretty sweet right? Check out the full channel to see the newest version.\n\nThis is the first edition of Wintergaten's marble machine, a fully mechanical one-man-band style of percussion instrument.\n\n### 2.  Robotic Drums:\n\nThe industrial design was done by [https://www.instagram.com/vega.dstudio/](https://www.instagram.com/vega.dstudio/)\n\nElectronics by [https://www.instagram.com/martingava/](https://www.instagram.com/martingava/)\n\n\u003CYouTube id=\"nuv0noWdfhY\" />\n\nRobotic drums using a solenoid - see what I mean about spectacle?\n\nYo quiro el producto. (It's been a few years since I've used Spanish, that is supposed to say \"I want this product\"). This is a midi controlled actuator / lever setup. I like this a lot.\n\n### 3. Abelton - Motors, Magnets and Motion: Electronic Music Instruments from the Physical World\n\n\u003CYouTube id=\"hJHwhb99Bzo\" />\n\nFascinating work from the intersection of art, mechanics, and sound.\n\n4\\. Jon Peck's [https://null.band/ ](https://null.band/%20) from [Rochester Maker Faire](https://rochester.makerfaire.com?ref=edwarddeaver.me):\n\n\u003CVimeo id=\"791238719\" />\n\nThose are piano strings.\n\nThis was a mixture of guitar/bass pickups and piezo mics on the bottom of the tube to capture resonance. This was very fun to play with. You can make some wild sounds on those strings.\n\n### Drum machines and Sequencers\n\nDrum machines like [Roland Tr-808](https://www.roland.com/us/promos/roland_tr-808/?ref=edwarddeaver.me), and sequencers like the [Arturia BeatStep](https://www.arturia.com/products/hybrid-synths/beatstep?ref=edwarddeaver.me) are staples in music production now, but let's look at the DIY examples.\n\n#### 1.  Check out this video on the first drum machine by Look Mum No Computer.\n\n\u003CYouTube id=\"fNgJcX2ckZQ\" />\n\n#### 2. Look Mum No Computer again showing how to make an Arduino [MIDI](https://cecm.indiana.edu/361/midi.html?ref=edwarddeaver.me) sequencer.\n\n\u003CYouTube id=\"9oGlCfwCoCw\" />\n\nSam's youtube channel is amazing.\n\n#### 3. BABS,  made by [Inciteco](https://www.inciteco.com/babs?ref=edwarddeaver.me).\n\n\u003Ciframe style=\"width: 100%; height: 738px; display: block; visibility: unset; max-height: 738px;\" name=\"__tt_embed__v41675081115601580\" sandbox=\"allow-popups allow-popups-to-escape-sandbox allow-scripts allow-top-navigation allow-same-origin\" src=\"https://www.tiktok.com/embed/v2/7185399945191492906?lang=en-US&referrer=https%3A%2F%2Fedwarddeaver.me%2Fblog%2Finitial-research-electromechanical-sound\" />\n\nThis is a huge multi-track step sequencer that they made for ComplexCon.\n\n#### [4. This is a step sequencer tutorial by Nick Poole on Sparkfun:](https://learn.sparkfun.com/tutorials/build-an-auduino-step-sequencer/all?ref=edwarddeaver.me)\n\n\u003CAstroImage src=\"assets/blog/52d44bcece395f7b1a8b4567-1.png\" alt=\"This is a step sequencer tutorial by Nick Poole on Sparkfun:\" width={1000} height={662} />\n\nArduino based sequencer by [NICK POOLE](https://www.sparkfun.com/users/207060?ref=edwarddeaver.me)\n\n#### 5. [Cassiopeia Ltd's](https://cassiopeia.hk/?ref=edwarddeaver.me) 4 step sequencer tutorial\n\n\u003CYouTube id=\"ozSdVL2EzR4\" />\n\nThis is a very nice tutorial on making a simple step sequencer. He also made a very high quality blog post to go along with it: [https://cassiopeia.hk/sequencer/](https://cassiopeia.hk/sequencer/)\n\n### odds and ends\n\nRecording the natural world can be done with contact microphones like the [Cortado Piezo mic](https://zeppelindesignlabs.com/product/cortado-balanced-piezo-contact-mic/?ref=edwarddeaver.me):\n\n\u003CYouTube id=\"p-8UPHHF714\" />\n\nSteppers themselves can make noise, if you've ever listened to a 3d printer you've heard their song. You can control these frequencies to make music, like Johnothan Kayne did with his interpretation of \"Fireflies\" by  Owl City.\n\n\u003CYouTube id=\"5YFajoHbdtM\" />\n\nThat's it for now. Till next time.","src/content/blog/initial-research-electromechanical-sound.mdx","e0bf014620af0533","initial-research-electromechanical-sound.mdx","more-power-more-directed-power",{"id":125,"data":127,"body":136,"filePath":137,"digest":138,"legacyId":139,"deferredRender":28},{"title":128,"description":129,"image":130,"date":131,"author":18,"categories":132,"tags":133}," More Power - Strike That - More Directed Power","The stepper motor isn't fast enough and I don't have the power/money budget to push it there. My takeaway from this short journey into mechatronics is the human body is truly an amazing creation.","assets/blog/pictureofthemotor.jpg",["Date","2023-04-01T00:00:00.000Z"],[117],[134,117,135],"arduino","creative-technology","**In action shot of the motor / kick pedal hitting a pizza pan**\n\nUh-oh the stepper motor isn't fast enough and I don't have the power/money budget to push it there (apparently things get fast at 90v). There must be a better way. My take away from this short journey into mechatronics is the human body is truly an amazing creation and no matter what I do, I won't beat it.\n\n## Why is speed an issue?\n\nThe Beats Per Minute of a song or how fast a song actually is. For example Everlong by Foo Fighters is 156 BPM, which breaks down to ( if there was a hit every beat ) 1 hit every 156 beats / 60sec = 2.5 beats per second. So in one second each beat takes 1000 ms / 2.5 = 400ms per beat. If you were hitting a drum your down stroke would be 200ms and up stroke 200 ms.  \n\n## How fast can my stepper motor go?\n\nThe speed test was setup with my motor attached to my Power Supply Unit at 30 volts and a limit of 5 amps (PSU rating). A side effect of running it at max power is heat, it gets hot (it's like touching a 150 watt incandescent bulb). I let it get up to speed and  using my iPhone slow-mo mode, 240 FPS recorded it.\n\nOnce done and imported into DaVinci Resolve Free, which only supports 60 FPS timelines (it cuts the frames out instead of time stretching them to fit), and counted 90 frames to make a revolution or 1.5 seconds or 1500 milliseconds. This assumes an already up to speed motor and does not take into account acceleration and deceleration times.  \n\n\u003CVimeo id=\"804975210\" />\n\n240 FPS recording of the stepper motor.\n\nAfter doing this I found this VLC plugin ([Time](https://addons.videolan.org/p/1154032?ref=edwarddeaver.me)) that can just display the time in milliseconds overtop the video so you don't need frame math and can handle high FPS video.\n\n## So now what?\n\nMore searching: I found this this tutorial by [Adafruit: MIDI Solenoid Drum Kit](https://learn.adafruit.com/midi-solenoid-drum-kit/circuit-diagram?ref=edwarddeaver.me) and this wonderful tutorial on a fully robotic drum set by [Randofo](https://www.instructables.com/member/randofo/?ref=edwarddeaver.me) on [Instructables](https://www.instructables.com/Arduino-Controlled-Robotic-Drum/?ref=edwarddeaver.me). The latter tutorial turned me onto car door lock actuators. It turns out finding actuators with a long enough stroke gets pretty expensive and these are pretty affordable.\n\nAlso, there are pneumatics that would do parts of the job well like speed and torque but are hampered by the noise of an air compressor. I guess you could mitigate that with sound proofing it or having lots of gas cylinders, but that doesn't seem very good for a public event.\n\nI picked up a couple Large push-pull solenoids and tested them. The solenoid is faster, a lot faster than the stepper motor. It can do a round trip in 100ms. Thank you to madbodger on the Adafruit Discord for helping me with the ULN2803 circuit.\n\n\u003CAstroImage src=\"assets/blog/IMG_4199.webp\" alt=\"  Circuit of ULN2803 on a bread board\" width={1500} height={2000} caption=\"  Circuit of ULN2803 on a bread board\" />\n\nThe trade off for this speed is torque, pull isn't even close to what that stepper could do. These work in a way, if I wanted to use test drum pads like the tutorial it'd be fine but I want spectacle. I want things moving. To achieve that I need something that had a longer stroke length to pull my kick drum pedal and that's where the longer stroke car lock solenoids came in.\n\nSo I drilled out the screw holding the pedal to the chain of the kick drum lever and zipped it down to a box to mount it all.\n\nAt 60 FPS it's hard to measure the speed but it's about 140 ms for the down stroke and 100 ms for the up stroke. This is a pull solonoid only. So it doesn't have an integrated spring like the push - pulls do, so I tuned the pull strength of the pedal to bring it back quickly and not impede for down stroke.\n\nSo here we are. A working solution.\n\n\u003CVimeo id=\"804973444\" />\n\nDrum hitting pizza pan.\n\nBut, the ULN2803 doesn't do well when sinking 5 amps into it, it can only take 500ma per channel so... relays it is.\n\n\u003CVimeo id=\"804952344\" />\n\nI have parts on order to get MIDI working. I'm conflicted if I'll implement it and just go with CV instead.\n\nTill then, good night.","src/content/blog/more-power-more-directed-power.mdx","70003dba6d724463","more-power-more-directed-power.mdx","making-of-midi-kickdrum-maker-faire",{"id":140,"data":142,"body":150,"filePath":151,"digest":152,"legacyId":153,"deferredRender":28},{"title":143,"description":144,"image":145,"date":146,"author":18,"categories":147,"tags":148},"Making of MIDI Kickdrum / Maker Faire Experience ","I wanted a project that would combine a few interests of mine and make for a cool exhibit at Maker Faire Syracuse. This was filled with firsts. First time exhibiting at Maker Faire, first time making a MIDI Instrument, first time making an electromechanical instrument first real MaxMSP patch.","assets/blog/drum (1).webp",["Date","2023-05-01T00:00:00.000Z"],[117],[134,149],"sound","I wanted a project that would combine a few interests of mine and make for a cool exhibit at Maker Faire Syracuse. This was filled with firsts. First time exhibiting at Maker Faire, first time making a MIDI Instrument, first time making an electromechanical instrument first real MaxMSP patch.\n\n\u003CVimeo id=\"815262283\" />\n\nI started this project after watching hours of Sam's journey from the Look Mum No Computer YouTube channel about making an old church organ controllable with MIDI. It planted a seed in me to make an electromechanical thing, and then seeing his visitor counter video just sent me on a journey. My initial electro mechanical research is in this [blog](https://edwarddeaver.me/blog/initial-research-electromechanical-sound/) and work with [stepper motors here.](https://edwarddeaver.me/blog/more-power-more-directed-power/)\n\n\u003CVimeo id=\"791238719\" />\n\nThis is Null.Band's resonance tube.\n\n\u003CYouTube id=\"8PwwRR8deHk\" />\n\nThis is Sam's journey with the organ.\n\nI started this a few months ago working on it off and on with the goal of having an interactive experience for [Syracuse Maker Faire](https://syracuse.makerfaire.com/?ref=edwarddeaver.me): \"We call it the Greatest Show (& Tell) on Earth - a family-friendly showcase of invention, creativity, and resourcefulness.\".\n\nThis project stretched my knowledge, and I had to rely on help from the Adafruit, ElectroSmith and Max Discord communities.\n\n\u003CAstroImage src=\"assets/blog/Screenshot 2025-09-16 at 10.21.49 PM.png\" width={1180} height={430} alt=\"Adafruit Electrosmith and MAX MSP logo\" />\n\n## Striking:\n\nAfter testing a variety of stepper motors and solenoids I found this [Instructable](https://www.instructables.com/Arduino-Controlled-Robotic-Drum/?ref=edwarddeaver.me) that recommended them. These have enough torque to pull a kick drum pedal but are not rated for continuous use so they get hot, and can start releasing some magic smoke. To alleviate this I reduced the spring tension on the kick drum pedal, so keep the amps down to 3 Amps drawn (every time it fired it drew 36 watts). The wood is from Home Depot, and the bolts in the actuator were replaced with longer ones from McMasterCar so they could be secured down.\n\n## MIDI/Max:\n\n\u003CAstroImage src=\"assets/blog/Screenshot-2023-04-06-at-5.53.58-AM.png\" alt=\"MAX MSP screenshot\" width={1600} />\n\nScreenshot of the Max patch\n\nAfter getting my actuating motion working I moved on to MIDI. I got an [Arturia KeyStep ](https://www.arturia.com/keystep/overview?ref=edwarddeaver.me)MIDI controller so to have a source of truth for the MIDI signals.\n\nTo route the MIDI signals I used [Cycling '74 Max](https://cycling74.com/products/max?ref=edwarddeaver.me). This was my first time really making a Max patch on my own that wasn't part of a tutorial. It's interesting and quirky especially if you come from a text-based programming background. It doesn't act like you would want sometimes, it is constrained but once you start to understand how it works the creation process gets easier as you purge your mind of comparing it to other languages. Overall, it just works. It was really easy to get started and whenever I had issues the community was amazing and helped me out. My Max program takes in MIDI and then splits it into 2 outputs. One checks if note 70 is pressed and then sends data to the Daisy Seed over USB MIDI, the other implements a delay to match the kick drum delay and has the Operating System make the keyboard notes. I can not express just how helpful the Max community is amazing and I want to thank Alex M, quail, BLAIR specifically.\n\n## Daisy:\n\nThe [Electrosmith Daisy Seed](https://www.electro-smith.com/daisy/daisy?ref=edwarddeaver.me) is such a cool little board and company. I chose this after searching around for boards that could easily do MIDI, and just become enveloped in the community and its capabilities. This board can do it all and I've only scratched the surface of it. This time around I used their [libDaisy](https://github.com/electro-smith/libDaisy?ref=edwarddeaver.me) library for C++ and USB MIDI for data transfer. This was also my first time outside of the Arduino ecosystem. Their Discord is awesome, thank you specifically to Nick // Infrasonic Audio, angrydan, ifranco, Takumi\\_Ogata, nii-nil and aidolon.\n\nA Daisy GPIO pin was connected to a relay board I had gotten from Sparkfun a while ago for a different project that didn't pan out. The relay turns on and off the 12v to the actuator. This relay itself could be a percussion instrument. It has such a loud click that paired with a piezo contact mic could be a cool little unit itself.\n\n## Maker Faire:\n\nThe drum part of the drum machine was put together the night before Maker Faire. After getting lost in the technicals I forgot to make a thing to hold actually drum. So I got a canvas from an art store and put some hooks in the box.\n\n\u003CAstroImage src=\"assets/blog/drum.jpg\" alt=\"Drum\" height={3024} />\n\nThe arm to hold the canvas and hooks was done the night before.\n\nThis was my first time presenting at Maker Faire and it was incredible! I got to see some friends from Rochester Maker Faire: @[lamemakes](https://www.instagram.com/lamemakes/?ref=edwarddeaver.me) (we were booth neighbors!), @[null.band](https://null.band/?ref=edwarddeaver.me) (the same guy from the inspiration blogs for this project), [Forge Gone Conclusions](https://edwarddeaver.me/blog/making-of-midi-kickdrum-maker-faire/fgconclusions.com). And I made new ones!  \n\nI didn't know what to expect so I made some posters to explain what I did, with a similar ethos to my [capstone posters](https://edwarddeaver.me/portfolio/computer-science-bachelors-capstone/). This was a good move as I got a few compliments on my booth design.\n\n\u003CAstroImage src=\"assets/blog/booth.jpg\" alt=\"  My booth at Syracuse Maker Faire. \" height={3024} caption=\"  My booth at Syracuse Maker Faire. \" />\n\nIt was great to be able to talk about the local arts scene here like The Little Room (RIP), The Noise Source (RIP), Wunder Bar (RIP). \n\nI met a guy who knew what Max was which was mind-blowing. He had been teaching his daughter music through [PureData](https://puredata.info/?ref=edwarddeaver.me), which was just so cool! And it was nice to talk to another person about DIY synths and the Daisy Seed.\n\n\u003CYouTube id=\"1o5Wasmd8yU\" />\n\nIntro to PureData. \n\nNote on children interacting with it: First, I didn't expect it would be as popular as it was with kids. Second, a lot can play the piano and some asked me actual music theory questions that I couldn't answer so I'll have to study up. I think children are a very good measure of UX and interactivity interest. It's a pretty clear-cut question for them, \"Interesting? Yes/No\".\n\nSnother note, have a friend with you or make some there who can cover your booth. Thank you to my parents and a friend who came out and LameMakes for covering my booth. Also, drink a lot of water; talking for 8 hours straight takes a toll on your vocal cords. And see other booths! I didn't take the time to check out all the ones I wanted to, there was a robot drawing pictures I didn't see, it reminded me of this project by [DeepLocal](https://www.deeplocal.com/natgeo-einsteins-chalkboard?ref=edwarddeaver.me) and [VTProDesign](https://vtprodesign.com/work/painted-by-everyone?ref=edwarddeaver.me).\n\nI can't wait to go back next year.\n\nBest,\n\nEdward","src/content/blog/making-of-midi-kickdrum-maker-faire.mdx","ec5ba30bc015a0af","making-of-midi-kickdrum-maker-faire.mdx","in-memorial-eyeo-and-changed-world-view",{"id":154,"data":156,"body":165,"filePath":166,"digest":167,"legacyId":168,"deferredRender":28},{"title":157,"description":158,"image":159,"date":160,"author":18,"categories":161,"tags":163}," In Memorial of Eyeo and a Changed World View","In 2022 I went to Eyeo, after 3 years of waiting, it was a transformative experience from meeting people in the coding space to having my worldview changed by city that values biking. That was also my last Eyeo as Eyeo is no longer.\n","assets/blog/IMG_0716.webp",["Date","2023-06-30T00:00:00.000Z"],[162],"conferences",[135,164],"minneapolis","In 2022 I went to Eyeo, after 3 years of waiting, it was a transformative experience from meeting people in the coding space to having my worldview changed by city that values biking. That was also my last Eyeo as Eyeo is no longer. If you are looking for previous talks they can be found [here](https://vimeo.com/eyeofestival?ref=edwarddeaver.me).\n\nI found out about Eyeo from working on [Processing](https://processing.org?ref=edwarddeaver.me) things in the Summer of my 2nd to last year of college and lurking in Creative Coding areas of the net. Then found you could do student volunteering to get a free ticket. Covid derailed those plans and Eyeo was canceled and I graduated. In 2021 there was no Eyeo, but in 2022 there was! This was the furthest I had traveled and what an experience it was. Coming off the back of struggles earlier in the year, this was a sign to myself of getting better and pushing a barrier.\n\n\u003CAstroImage src=\"assets/blog/IMG_0656.jpeg\" alt=\"Bald man with beard takes a selfie\" width={2667} height={2000} />\n\nI met friends from the internet IRL.\n\nI met new friends. I went on a bird tour by Jer Thorp and learned about epaulets on Red-Winged Blackbirds (the red spots) and these Chimney swifts that make nests on chimneys and sides of buildings.\n\nI saw a side of an industry totally foreign to me.\n\nAttended some cool talks ([Nightingale](https://nightingaledvs.com/?ref=edwarddeaver.me) is a data magazine and not a talk):\n\n\u003Cdiv class=\"gallery\">\n  \u003CAstroImage src=\"/assets/blog/IMG_0642-1.jpeg\" alt=\"EYEO\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_0643-1.jpeg\" alt=\"EYEO\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_0649-1.jpeg\" alt=\"EYEO\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_0656.jpeg\" alt=\"EYEO\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_0669-1.jpeg\" alt=\"EYEO\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_0670.jpeg\" alt=\"EYEO\" width={970} height={500} />\n\n\u003C/div>\n\n\n\nSaw some cool art (these were at the Walker Art Center):\n\n\u003Cdiv class=\"gallery\">\n  \u003CAstroImage src=\"/assets/blog/IMG_0604.jpeg\" alt=\"Walker Art Center\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/IMG_0609.jpeg\" alt=\"Walker Art Center\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/IMG_0615.jpeg\" alt=\"Walker Art Center\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/IMG_0621.jpeg\" alt=\"Walker Art Center\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/IMG_0624_1.jpeg\" alt=\"Walker Art Center\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/IMG_0629.jpeg\" alt=\"Walker Art Center\" width={970} height={500} />\n  \u003CAstroImage src=\"/assets/blog/IMG_0631.jpeg\" alt=\"Walker Art Center\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_0643-1.jpeg\" alt=\"EYEO\" width={970} height={500} />\n\n  \u003CAstroImage src=\"/assets/blog/IMG_0649-1.jpeg\" alt=\"EYEO\" width={970} height={500} />\n\n\n\u003C/div>\n\n\nI saw a gay couple for the first time in an ad, that was cool \n\n\u003CAstroImage src=\"/assets/blog/IMG_0727.jpeg\" alt=\"gay couple on plane ad\" width={970} height={500} />\n\n\n\nBut more fundamentally I saw what a city that values pedestrians is like. I saw a city that had protected bike lanes everywhere. I miss Minneapolis and will be visiting there again soon... I hope.\nA real protected bike lane.\n\n\u003CAstroImage src=\"/assets/blog/IMG_0600.jpeg\" alt=\"protected bike lane extension of sidewalk\" width={970} height={500} />\n\nThis experience laid the groundwork in my heart to establish a new company (shhh it's early days yet) and established a new way of thinking.\n\nGoodbye, Eyeo. Goodbye Minneapolis, we will see each other soon I can feel it.","src/content/blog/in-memorial-eyeo-and-changed-world-view.mdx","9499aab221a465d3","in-memorial-eyeo-and-changed-world-view.mdx","author",["Map",171,172],"edward-deaver",{"id":171,"data":173,"body":176,"filePath":177,"digest":178,"legacyId":179,"deferredRender":28},{"title":18,"subtitle":174,"image":175},"Creator","/assets/author/edward-deaver.jpg","Edward Deaver is a creative technologist that enjoys spending his time outside climbing, running, hiking, and biking.","src/content/author/edward-deaver.mdx","168789fa5087d319","edward-deaver.mdx","pages",["Map",182,183,202,203,219,220,227,228,245,246,169,253,262,263,277,278,286,287,295,296],"-index-2",{"id":182,"data":184,"filePath":200,"digest":201,"deferredRender":28},{"subtitle":185,"title":186,"image":187,"metadata":188,"description":191,"banner_button_label":192,"social_links":193,"recent_posts":197},"Hello, I'm","Daniel Alberto","/assets/profile.jpg",{"title":189,"description":189,"author":189,"image":189,"keywords":189,"noindex":190,"canonical":189},"",false,"A writer based in New York. I'm interested in all things tech, science, and photography things. I also like to yo-yo in my free time.","Explore Posts",{"enable":28,"twitter":194,"facebook":195,"linkedin":196},"https://twitter.com/daniel_alberto","https://github.com/danielalberto","https://linkedin.com/in/danielalberto",{"enable":28,"title":198,"button_label":199},"Recent Posts","View all posts","src/content/pages/-index-2.mdx","1863125d17c2e213","-index-3",{"id":202,"data":204,"filePath":217,"digest":218,"deferredRender":28},{"metadata":205,"social_links":208,"featured_posts":212,"recent_posts":214},{"title":206,"description":176,"author":18,"image":189,"keywords":207,"noindex":190,"canonical":189},"Edward Deaver - Portfolio","arduino, creative coding, processing, p5.js, openFrameworks, C++, interactive art",{"enable":28,"linkedin":209,"github":210,"instagram":211},"https://www.linkedin.com/in/edwardcdeaveriv/","https://www.github.com/edwarddeaver","https://www.instagram.com/edwarddeaver.me/",{"enable":28,"title":213},"Featured Works",{"enable":28,"title":215,"button_label":216},"Recent Works","View all works","src/content/pages/-index-3.mdx","325aaf734e5a48dc","-index",{"id":219,"data":221,"filePath":225,"digest":226,"deferredRender":28},{"title":222,"metadata":223,"recent_posts":224},"Taking control of your daily life is easy when you know how!",{"title":189,"description":189,"author":189,"image":189,"keywords":189,"noindex":190,"canonical":189},{"enable":28,"title":215,"button_label":216},"src/content/pages/-index.mdx","759c60c697ee128a","about",{"id":227,"data":229,"filePath":243,"digest":244,"deferredRender":28},{"draft":190,"title":230,"metadata":231,"intro":233,"our_writers":241},"About",{"title":189,"description":232,"author":189,"image":189,"keywords":189,"noindex":190,"canonical":189},"This a dummy meta description of about page",{"title":234,"images":235,"description":240},"Edward Deaver – Creative Technologist | Builder | Cyclist",[236],{"src":175,"width":237,"height":238,"grid_class":239},620,346,"md:col-6","Edward Deaver is a creative technologist interested in the human aspect of technology and making the spaces we inhabit—physical and digital—better. With a versatile skill set and a passion for movement, he builds innovative, people-focused systems that span from immersive installations to intelligent backend APIs.\n\nEdward thrives at the intersection of hardware and software. He's fluent in Python, JavaScript, and C++, and works comfortably across modern frameworks like FastAPI, React, and openFrameworks. His toolbox includes Adobe Creative Suite, Figma, Blender, and Max/MSP, as well as hands-on tech like Raspberry Pi, Arduino, and MIDI devices.\n\nWhether he's building AR experiences, reverse-engineering undocumented devices, or deploying cloud-native systems with AWS Lambda and API Gateway, Edward brings a builder’s mindset and a collaborative spirit to every project. He’s no stranger to AI tools like ChatGPT and Codeium, using them to synthesize documentation, accelerate development, and navigate the rough edges of under-documented systems—always with version control as a safety net.\n\nExternal Projects & Hackathons\n\nMIT Reality Hack 2023: Built a VR physics learning experience for Oculus Quest 2, integrating Csound and ESP32 hardware.\n\nHack Upstate Fall 2020: Created a TensorFlow/OpenCV mail-truck detector that triggered email notifications based on real-world activity.\n\nCity of Syracuse Innovation Team (Intern): Developed a real-time snowplow mapping prototype using MapQuest geocoding, Verizon GPS data, Tile38, and Leaflet.\n\nProfessional Experience\n\nFull Stack Web Developer, Syracuse University\nDeveloped augmented reality experiences and social filters using TikTok Effect House. Built APIs and automation pipelines using FastAPI, Node.js, and GitHub Actions to eliminate multi-day manual processes.\n\nSplunk Engineer, Evolent Health\nEnhanced security operations through log analysis, data searching with Splunk and Cribl (similar to FluentDB), and Python apps for actionable insights.\n\nAthletic & Personal Interests\n\nOutside of work, Edward is deeply involved in competitive cycling, running, and rock climbing. His love for bikes goes beyond riding—he builds them, advocates for safer cycling infrastructure, and stays active in his local riding community.\n\nRecently attempted the Irreverent Road Ride (IRR) in Vermont, riding a rebuilt LeMond Poprad Disc.\n\nHas logged over 3,600 miles on Strava and uses tools like RideWithGPS for route planning and climb analysis.\n\nCombines tech and fitness: analyzing performance data, exploring training plans, and always looking for ways to improve the digital side of physical activity.\n\nEnjoys working with others—whether collaborating with external contractors or bouncing ideas off fellow technologists and riders.\n\nServices\n\nAugmented Reality\n\nLED Installation\n\nInteractive Work\n\nAPI Development & Automation\n\nWeb Mapping & Real-time Data Visualization\n",{"enable":190,"title":242},"Our Writers","src/content/pages/about.mdx","12bb7cc4af07eeaa","archive",{"id":245,"data":247,"filePath":251,"digest":252,"deferredRender":28},{"draft":190,"title":248,"metadata":249},"Archive",{"title":189,"description":250,"author":189,"image":189,"keywords":189,"noindex":190,"canonical":189},"This a dummy meta description of archive page","src/content/pages/archive.mdx","05ce57b126c8aca8",{"id":169,"data":254,"filePath":260,"digest":261,"deferredRender":28},{"draft":190,"title":255,"metadata":256,"recent_posts":258},"Author",{"title":189,"description":257,"author":189,"image":189,"keywords":189,"noindex":190,"canonical":189},"This a dummy meta description of author page",{"enable":28,"title":198,"visible_posts_count":259,"button_label":199},3,"src/content/pages/author.mdx","e5cc920e96a23d51","contact",{"id":262,"data":264,"filePath":275,"digest":276,"deferredRender":28},{"draft":190,"title":265,"metadata":266,"contact_form":269},"Contact",{"title":267,"description":268,"author":18,"image":189,"keywords":189,"noindex":190,"canonical":189},"Edward Deaver - Contact","Contact Edward Deaver for inquiries, collaborations, or to learn more about his work in creative technology and interactive art.",{"title":270,"description":271,"contact_info_title":272,"contact_email":273,"form_button_text":274},"Contact Edward Deaver","I'm here to help and answer any question you might have. I look forward to hearing from you","Hate forms? \u003Cbr> Write an email or make a call","respond@edwarddeaver.me","Send","src/content/pages/contact.mdx","ceb595ee7d0eaf0f","tags",{"id":277,"data":279,"filePath":284,"digest":285,"deferredRender":28},{"draft":190,"title":280,"metadata":281,"recent_posts":283},"Tags",{"title":189,"description":282,"author":189,"image":189,"keywords":189,"noindex":190,"canonical":189},"This a dummy meta description of tags page",{"enable":28,"title":198,"visible_posts_count":259,"button_label":199},"src/content/pages/tags.mdx","88b09b1b34488727","privacy",{"id":286,"data":288,"body":292,"filePath":293,"digest":294,"deferredRender":28},{"draft":190,"title":289,"metadata":290},"Privacy",{"title":189,"description":291,"author":189,"image":189,"keywords":189,"noindex":190,"canonical":189},"Privacy & Policy","**Last updated on October 08, 2024**\n\n\nBelow are our dummy [Privacy & Policy](#!), which outline a lot of legal goodies, but the bottom line is it’s our aim to always take care of both you, as a customer, or as a seller on our platform.\n\n### Licensing Policy\n\nBy visiting and/or taking any action on our template, you confirm that you are in agreement with and bound by the terms outlined below. These terms apply to the website, emails, or any other communication.\n\n#### Here are terms of our Standard License:\n* The Standard License grants you a non-exclusive right to make use of template you have purchased.\n* You are licensed to use the Item to create one End Product for yourself or for one client (a “single application”), and the End Product can be distributed for Free.\n\n#### If you opt for an Extended License:\n* You are licensed to use the Item to create one End Product for yourself or for one client (a “single application”), and the End Product maybe sold or distributed for free.\n\n### Additional Policy\nBy visiting and/or taking any action on our template, you confirm that you are in agreement with and bound by the terms outlined below. These terms apply to the website, emails, or any other communication.\n\n* You have 2 days to evaluate your purchase. If your purchase fails to meet expectations set by the seller, or is critically flawed in some way, contact Bootstrap Themes and we will issue a full refund pending a review.","src/content/pages/privacy.mdx","45094026e5d73404","categories",{"id":295,"data":297,"filePath":303,"digest":304,"deferredRender":28},{"draft":190,"title":298,"metadata":299,"recent_posts":302},"Categories",{"title":298,"description":300,"author":18,"image":189,"keywords":301,"noindex":190,"canonical":189},"Categories of blog/portfolio posts","Categories, Tags",{"enable":28,"title":198,"visible_posts_count":259,"button_label":199},"src/content/pages/categories.mdx","e56b49649dcdcf21","portfolio",["Map",307,308,323,324,338,339,353,354,368,369,382,383,395,396,413,414,428,429,444,445,459,460],"ny-times-election-on-flip-digit",{"id":307,"data":309,"body":319,"filePath":320,"digest":321,"legacyId":322,"deferredRender":28},{"title":310,"description":311,"image":312,"date":313,"author":18,"categories":314,"tags":316},"The NY Times Election data on an AlfaZeta FlipDigit display.","Click Clack - a terrifying result emerged. ","assets/blog/trumpvharris.webp",["Date","2024-11-04T05:00:00.000Z"],[315],"data-visualization",[317,318],"mechanical","javascript","\u003CVimeo id=\"1027074047\" />\n\nTimelapse with sound added back in of the night.\n\nClick Clack… it updated. \n\nA days before the election: At Syracuse’s Open Data Day met someone (not sure they want to be named) that told me The NY Times in years past published their election results via a JSON file that gets queried every couple minutes. I found a link to last years and built a script for my display around it. If that data is published in a similar way this year my script should work for this years election. \n\nElection night: The NY Times's election site is powered by \\~5 json files and those JSON files are one of the few ways to get election data for free. At 6pm when the new data dropped, I changed the data structure to fit it.\n\n\u003CLinkPreview id=\"https://github.com/EdwardDeaver/2024_Presidential_Election_Scraping\" />","src/content/portfolio/ny-times-election-on-flip-digit.mdx","5feb4c8746cb6b7b","ny-times-election-on-flip-digit.mdx","empoweredar-mit-reality-hack-2024",{"id":323,"data":325,"body":334,"filePath":335,"digest":336,"legacyId":337,"deferredRender":28},{"featured":28,"title":326,"description":327,"image":328,"date":329,"author":18,"categories":330,"tags":332},"EmpoweredAR","EmpoweredAR leverages the ability of Xreal eyewear to generate meshes of the physical environment and combined with accurate and reliable distance tracking, can provide wearers with an auditory map.","assets/blog/gallery.webp",["Date","2024-01-28T05:00:00.000Z"],[331],"XR",[104,333],"csound","\u003CVimeo id=\"907226882\" />\n\n## Inspiration\n\nWe wanted to tackle something that revolved around accessibility and how AR technology could be more than just entertainment or educational. One of the mentors at the hack is visually impaired, and motivated us to pursue a solution for him and others like him.\n\n## What it does\n\nUsing the Xreal lenses that generate a digital mesh, we established that we could provide the user with an auditory ping. This gives them information about distance to objects and the space around them. Not only are wearers able to build a larger spatial map, but they can safely navigate around obstacles in their path.\n\nHow we built it\n\nWe utilized Unity audio for the audio playback, TensorFlow and YOLOV3 for object detection, and the XREAL glasses depth data for a mesh and raycasting to determine object distance.\n\n## Challenges we ran into\n\nThe XREAL lenses do not have an RGB camera, and therefore, we cannot differentiate between obstacles or a wall that they use to walk through spaces.\n\nWorking on the same project in git with Unity posed issues when merging and required careful pruning of files.\n\nAccomplishments that we're proud of\n\nGetting object detection working when user moves head.\n\nWhat we learned\n\nXREAL development TensorFlow/YOLOv3 in Unity Spatial Audio in Unity\n\n## What's next for EmpoweredAR\n\nImplement object detection with AI support and voice audio so that users can complete tasks such as locate a handrail and safely navigate down a set of stairs. Our current build is optimal only for indoor spaces, and would have difficulty navigating open areas or dynamic objects in space.\n\nProject Abstract\n\nEmpoweredAR leverages the ability of Xreal eyewear to generate meshes of the physical environment and combined with accurate and reliable distance tracking, can provide wearers with an auditory map.\n\nCode Repository\n\n[https://codeberg.org/reality-hack-2024/TABLE\\_72](https://codeberg.org/reality-hack-2024/TABLE_72)\n\n## Purpose\n\nThe general population’s current perspective of VR and AR products is that they are for education or entertainment purposes, but we wanted to set out to demonstrate that they could be utilized to better connect people to their environment.\n\nOur concept, EmpoweredAR, intends to help the visually impaired navigate the spaces they inhabit, and allow them to gain back the confidence to explore their world.\n\nHow we do it\n\nEmpoweredAR leverages the ability of Xreal eyewear to generate meshes of the physical environment and combined with accurate and reliable distance tracking, can provide wearers with an auditory map. As they approach an object that they may collide with, a tone is created and the intervals between the tones are shortened relative to their distance from the object, giving them very explicit information about the space. Although we focused on collision avoidance, we quickly realized that this helped to illuminate clear spaces; distinguishing the two can help users generate a mental image of their space.\n\n## Limitations\n\nBecause the Xreal Air 2 Ultra eyewear did not have internal RGB cameras, we did not have object identification, and could therefore not guide users to objects that they did not want to avoid, such as handrails on stairs, or towards a water fountain to refill their bottle. We understood that with further development and hardware advances, this would be a clear next step of design. We considered also leveraging AI for this task, and could shorten our development time significantly.\n\n## Key takeaways\n\nAs our team learned to work together and wanted to put our best foot forward, we leveraged our strengths to tackle a problem that we had no prior knowledge of.\n\nEdward learned native Unity sound design/scripting this year, vs CSound last year, and also some basic sound design to complete our project. He mainly has experience with web development/Arduino.\n\nHakan had experience developing ML models before however, it was his first time using Unity for running AI models. He managed to train/deploy an object detection model for this project.\n\n## Technology Stack:\n\nHardware: XReal 2 Ultra Software: Unity / Unity Audio Tooling Steam Audio GarageBand (Steinway Grand Piano) Audacity YOLOv3 Tensorflow Adobe Premier/After Effects\n\nTeam Members:\n\nAlbert Zhou Albert is a Junior at the University of Missouri studying Information Technology. His research is focused on the impact that VR has on education and mental health. In his spare time, he enjoys working with servers and computer networking.\n\nLorraine Cruz Lorraine is a Designer & Critical Theorist based in NYC. They’re currently pursuing a Masters in Design & Technology at Parsons School of Design. Their research centers decolonial theory and seeks to reimagine social systems.\n\nEdward Deaver, IV Edward is a creative technologist from Syracuse, NY. This is his 2nd Reality Hack. He has a computer science Bachelor’s degree from Le Moyne College.\n\nHakan Otal Hakan is a researcher from Albany, NY. He is currently pursuing PhD at SUNY Albany. His research focuses on graph neural networks and network security.\n\nLarry Chan\n\nThank you so much to the mentors and sponsors for whom this project would not have been possible:\n\nPatrick from BenVision Chris McNally Gregory Osborne Shirly Spikes Wei at XREAL\n\nBuilt With\n\nadobeaftereffects\n\nadobeillustrator\n\ngarageband\n\nsteamaudio\n\ntensorflow\n\nunity\n\nyolov3","src/content/portfolio/empoweredar-mit-reality-hack-2024.mdx","41ad8367f99252a0","empoweredar-mit-reality-hack-2024.mdx","augmented-reality-otto-tiktok-filter",{"id":338,"data":340,"body":349,"filePath":350,"digest":351,"legacyId":352,"deferredRender":28},{"title":341,"description":342,"image":343,"date":344,"author":18,"categories":345,"tags":346}," TikTok Augmented Reality Otto Filter","Working with a multi-faceted team I created an AR filter for TikTok that allowed you to place Otto (Syracuse Universities’ mascot) in 3D space with you.","assets/blog/Screenshot+2023-08-04+at+9.10.25+AM.webp",["Date","2023-03-08T05:00:00.000Z"],[89],[347,348],"ar","tiktok","Working with a multi-faceted team I created an AR filter for TikTok that allowed you to place Otto (Syracuse Universities’ mascot) in 3D space with you.\n\nModel created by: Jeremy Walker.\n\n\n### Here are some videos of people using the filter:\n#### @soleyliboy\n\u003Cvideo controls>\n  \u003Csource src=\"/media/soleyliboy_tiktok.mp4\" type=\"video/mp4\" />\n  Your browser does not support the video tag.\n\u003C/video>\n\n#### @syracuseu\n\u003Cvideo controls>\n  \u003Csource src=\"/media/scotthanson_tiktok.mp4\" type=\"video/mp4\" />\n  Your browser does not support the video tag.\n\u003C/video>\n\n#### @maddydevera\n\u003Cvideo controls>\n  \u003Csource src=\"/media/maddy_tiktok.mp4\" type=\"video/mp4\" />\n  Your browser does not support the video tag.\n\u003C/video>","src/content/portfolio/augmented-reality-otto-tiktok-filter.mdx","2e4ebc5a6352e298","augmented-reality-otto-tiktok-filter.mdx","mit-reality-hack-2023",{"id":353,"data":355,"body":364,"filePath":365,"digest":366,"legacyId":367,"deferredRender":28},{"featured":28,"title":356,"description":357,"image":358,"date":359,"author":18,"categories":360,"tags":361},"MIT Reality Hack 2023: Team Amadeus","Amadeus is an interactive application that teaches you about waveforms via a repurposed Guitar Hero controller and an ESP32 connected to Unity via Bluetooth. Looking into the VR glasses, the Quest 2, you are immersed in a sea of particles visualizing the transformations of modulated waveforms.","assets/blog/IMG_3641.webp",["Date","2023-01-15T05:00:00.000Z"],[89],[362,333,134,363],"audio","unity","\u003CAstroImage src=\"assets/blog/IMG_3641.webp\" alt=\"Guitar Hero controller on a desk. There is a circuit board on the left side of it with an arduino. Behind that is a laptop with a 3d world on it displaying sound waves and a Quest 2.\" width={2000} height={1500} />\n\n##\n\n## Team Amadeus\n\nA tool for educating young people on the science of sound with hardware and VR.\n\nIn 2.5 days a team that didn't know each other made an experience that left people with smiles on faces.\n\nThis was our submission video to the judges. \n\n\u003CYouTube id=\"es4Zc46btho\" />\n\n## Amadeus in action\n\n\u003CYouTube id=\"EnQNOCDVHaA\" />\n\n## Harmonix: \n\nHere is a video of Eric Malafeew, the Lead Architect and Engineering lead of Harmonix ( the company that made Guitar Hero), using it at the public expo:\n\n\u003CVimeo id=\"790120983\" />\n\n## Team:\n\n* [Edward Deaver, IV](https://www.linkedin.com/in/edwardcdeaveriv/?ref=edwarddeaver.me) - Initial sensor testing, Github coordination, Unity Csound Synthesizer, video production.  \n* [Dana Sy-Ching](https://www.linkedin.com/in/danasyching/?ref=edwarddeaver.me) - Hardware testing, wiring, input code\n* [Yash Goyal ](https://www.linkedin.com/in/yash-a-goyal/?ref=edwarddeaver.me)- Hardware testing, wiring, input code\n* [Whitt Sellers ](https://www.linkedin.com/in/whitt-sellers/?ref=edwarddeaver.me)- Unity VFX Graph particle system and lead merging all of our code together in Unity.\n* [Jacob Woods](https://www.linkedin.com/in/jacob-woods-4b41b5215/?ref=edwarddeaver.me) - Bluetooth code on ESP32 and communication between it and Unity.\n\n\u003CAstroImage src=\"assets/blog/team_photo2.jpg\" alt=\"Team photo\" width={1600} class=\"\" caption=\"\" height={1200} />","src/content/portfolio/mit-reality-hack-2023.mdx","331fd39da7e11e02","mit-reality-hack-2023.mdx","interactive-smart-city-display-explanation",{"id":368,"data":370,"body":378,"filePath":379,"digest":380,"legacyId":381,"deferredRender":28},{"title":371,"description":372,"image":373,"date":374,"author":18,"categories":375,"tags":376},"Interactive Smart City Display Explanation","For my computer science capstone project, I created an IoT installation with the goal of making smart technology tangible and less intimidating to the public, for the Innovation Office at the City of Syracuse, (a small, start-up-like office in City government).","assets/blog/Installation-2.webp",["Date","2020-01-18T05:00:00.000Z"],[52,20],[377,134,54,58,55],"civics","For my computer science capstone project, I created an IoT installation with the goal of making smart technology tangible and less intimidating to the public, for the Innovation Office at the City of Syracuse, (a small, start-up-like office in City government).\n\n[Read the making of here.](https://edwarddeaver.me/blog/the-making-of-bachelors-capstone-project/)\n\n[Code repository is here. ](https://github.com/EdwardDeaver/SyracuseInnovationLEDProject)\n\n\u003CVimeo id=\"1119283886\" />\n\n## Video of openFrameworks app running: \n\nTurn the sound on for tones generated at certain distance stages. openFameworks app controlling LEDs via OPC and FadeCandy on Raspberry Pi. \n\n\u003Cvideo controls>\n  \u003Csource src=\"/media/demo-openframeworks.mp4\" type=\"video/mp4\" />\n\n  Your browser does not support the video tag.\n\u003C/video>\n\nopenFrameworks application running on the Raspberry Pi. \n\n## Video of the website receiving data:\n\nThis was a Bootstrap website running on a NodeJS server on Heroku, that used SocketIO to receive data. \n\n\u003Cvideo controls>\n  \u003Csource src=\"/media/nodejs-website-demo.mp4\" type=\"video/mp4\" />\n\n  Your browser does not support the video tag.\n\u003C/video>\n\n##\n\n## Posters that I made: \n\n\u003CAstroImage src=\"assets/blog/arduino.png\" alt=\"This is a &#x22;microcontroller&#x22; It's a device that is told to do one thing and they do it forever. They are similar to how your cell phone works but instead of being able to run multiple apps at once, they can only run 1 app.\" width={2500} height={2001} caption=\"This is a &#x22;microcontroller&#x22; It's a device that is told to do one thing and they do it forever. They are similar to how your cell phone works but instead of being able to run multiple apps at once, they can only run 1 app.\" />\n\n\u003CAstroImage alt=\"  FadeCandy: this tells the light strip which individual lights to light up and what color. It's like a remote for the lights but controlled from the computer.\" src=\"assets/blog/fadecandy6.jpg\" width={2500} height={2001} caption=\"FadeCandy: this tells the light strip which individual lights to light up and what color. It's like a remote for the lights but controlled from the computer.\" />\n\n\u003CAstroImage src=\"assets/blog/neopixels.jpg\" alt=\"  NeoPixels: These lights are like Christmas lights. Each light can be changed to a different color.\" width={2500} height={2001} caption=\"  NeoPixels: These lights are like Christmas lights. Each light can be changed to a different color.\" />\n\n\u003CAstroImage alt=\"  Raspberry Pi: this is a very tiny computer. It works just like your laptop but is super compact. The computer runs a program that listens for the Arduino to say something - then tell the lights to light up.\" src=\"assets/blog/raspberrypi4.jpg\" width={2500} height={2001} caption=\"  Raspberry Pi: this is a very tiny computer. It works just like your laptop but is super compact. The computer runs a program that listens for the Arduino to say something - then tell the lights to light up.\" />\n\n\u003CAstroImage src=\"assets/blog/distancesensor.jpeg\" alt=\"  Distance Sensor: this allows the Arduino to tell how close you are. This is like motion detectors on garage lights or automatic doors at your favorite retail store.\" width={2500} height={2001} caption=\"  Distance Sensor: this allows the Arduino to tell how close you are. This is like motion detectors on garage lights or automatic doors at your favorite retail store.\" />","src/content/portfolio/interactive-smart-city-display-explanation.mdx","c98f4ece2995fbae","interactive-smart-city-display-explanation.mdx","control-my-lights",{"id":382,"data":384,"body":391,"filePath":392,"digest":393,"legacyId":394,"deferredRender":28},{"title":385,"description":386,"image":387,"date":388,"author":18,"categories":389,"tags":390},"Control My Lights","An interactive light installation in my home controlled a website, Twitch chat, YouTube chat. This was streamed over 3 weeks, and had 220 individual active users (those that sent a command), and 2093 Twitch live views (310 via Twitch and 895 from external sources).","assets/blog/frame2.jpg",["Date","2020-10-01T04:00:00.000Z"],[52],[54,58,57,55],"An interactive light installation in my home controlled a website, Twitch chat, YouTube chat. This was streamed over 3 weeks, and had 220 individual active users (those that sent a command), and 2093 Twitch live views (310 via Twitch and 895 from external sources).\n\n\u003CVimeo id=\"783520370\" />\n\n[Code can be found here. ](https://github.com/EdwardDeaver/ControlMyLights?ref=edwarddeaver.me)\n\nNote: YouTube chat related code not published. \n\nDescription: During the Summer of 2020, I launched controlmylights.net, an interactive light exhibit powered by a complex construction of NodeJS streaming data, Python, Redis, MongoDB, openFrameworks, ReactJS and Arduino controlling DIY LED tube lights. Chat data was gathered via Twitch’s API, and a Selenium scraping program for YouTube’s chat. It used Redis for an internal queue and pub/sub, as well as a MongoDB (NoSQL) database to store color commands. The program was architected using a Message Oriented Middleware paradigm with Node.JS as the main language. I created a website with buttons for every color that the light application supported using ReactJS and originally designed it in Figma. That project raised funds for Feeding America with several active users and over 2,000 Twitch views.\n\n### This was controlmylights.net:\n\n\u003Cvideo controls>\n  \u003Csource src=\"/media/controlmylightnet.mp4\" type=\"video/mp4\" />\n\n  Your browser does not support the video tag.\n\u003C/video>\n\n### This was the end: \n\n\u003Cvideo controls>\n  \u003Csource src=\"/media/endoftheset.mp4\" type=\"video/mp4\" />\n\n  Your browser does not support the video tag.\n\u003C/video>","src/content/portfolio/control-my-lights.mdx","00c707075acf9473","control-my-lights.mdx","midi-kick-drum",{"id":395,"data":397,"body":409,"filePath":410,"digest":411,"legacyId":412,"deferredRender":28},{"title":398,"description":399,"image":400,"date":401,"author":18,"categories":402,"tags":405},"MIDI Electro-Mechanical Kick Drum","I made a MIDI-controlled kick drum out of a car door lock actuator, relay, kick drum pedal, Daisy Seed (STM32), and orchestrated it with Cycling '74 MaxMSP.","assets/blog/drum-1.jpg",["Date","2023-05-14T04:00:00.000Z"],[403,404],"Interactive","Music",[406,407,408],"esp32","DaisySeed","MaxMSP","I made a MIDI-controlled kick drum out of a car door lock actuator, relay, kick drum pedal, Daisy Seed (STM32), and orchestrated it with Cycling '74 MaxMSP. [Code is here](https://github.com/EdwardDeaver/Electro-Mechanical-Kick-Drum).\n\n\u003CVimeo id=\"815262283\" />\n\nThe background music was made from the project recordings. \n\nPhoto from the faire:\n\n\u003CAstroImage src=\"assets/blog/makerfaire2.jpg\" width={2000} height={1324} alt=\"I made a MIDI-controlled kick drum out of a car door lock actuator, relay, kick drum pedal, Daisy Seed (STM32), and orchestrated it with Cycling '74 MaxMSP.\" caption=\"My booth at Maker Faire\" />","src/content/portfolio/midi-kick-drum.mdx","8d75ecfcddee27d2","midi-kick-drum.mdx","glsl-shader-experiments",{"id":413,"data":415,"body":424,"filePath":425,"digest":426,"legacyId":427,"deferredRender":28},{"title":416,"description":417,"image":418,"date":419,"author":18,"categories":420,"tags":422},"GLSL Shader Workshop Results","These are experiments from taking classes via Codame taught by Char Stiles. These are running in The Force an online GLSL shader live coding platform.","assets/blog/Screenshot+2023-08-07+at+7.14.32+PM.png",["Date","2020-06-30T04:00:00.000Z"],[421],"graphics",[423],"glsl","These are experiments from taking classes via [Codame](https://codame.com/) taught by [Char Stiles](http://charstiles.com/). These are running in[ The Force ](https://shawnlawson.github.io/The_Force/)an online GLSL shader live coding platform. \n\n## Pieces:\n\n### Corduroy Squiggles\n\n\u003CVimeo id=\"852507259\" />\n\n### Entering Darkness\n\n\u003CVimeo id=\"852508021\" />\n\n### Entering Darkness 2\n\n\u003CVimeo id=\"852509020\" />\n\n### Purple Glitch\n\n\u003CVimeo id=\"852510018\" />\n\n###\n\n### Rainbow Glitch\n\n\u003CVimeo id=\"852510487\" />","src/content/portfolio/glsl-shader-experiments.mdx","96cce0413224bdf6","glsl-shader-experiments.mdx","syracuse-snow-plow-mapping",{"id":428,"data":430,"body":440,"filePath":441,"digest":442,"legacyId":443,"deferredRender":28},{"title":431,"description":432,"image":433,"date":434,"author":18,"categories":435,"tags":436}," Syracuse Snow Plow Map","I produced a proof of concept snow plow mapping website for The City of Syracuse.","assets/blog/Proposedclientmap12.png",["Date","2019-03-11T04:00:00.000Z"],[20],[437,438,439],"gis","open-data","real-time","I produced a proof of concept snow plow mapping website for The City of Syracuse.\n\nA high-level overview of this project can be found on the Syracuse Innovation Team blog [here](http://www.innovatesyracuse.com/blog/snowplowmap) and a more technical version [here](http://www.innovatesyracuse.com/blog/snowplowmapdevelopment?ref=edwarddeaver.me).\n\n\u003CAstroImage src=\"assets/blog/snow+plow+map.png\" alt=\"A map of the city of Syracuse with the streets colored of when they were plowed. \" width={599} height={505} caption=\"A map of the city of Syracuse with the streets colored of when they were plowed. \" />\n\n## Introduction\n\nIn the beginning of 2018 Office of Accountability Performance and Innovation held a [Civic Hackathon](https://ischool.syr.edu/landing-pages/civic-hackathon-plowing-through-the-data) in partnership with Syracuse University’s iSchool to find insights on plow data. This project was a continuation of the mission of that hackathon to tackle our snow problem. The City of Syracuse’s snow plow mapping application is an in-house project, created by me (Edward Deaver, IV), a computer science intern with the City. \n\nSnow plow mapping applications have been implemented by other municipalities within the region, and throughout the country:  \n\n*     Rochester, New York: [https://gis.cityofrochester.gov/plowtrax/](https://gis.cityofrochester.gov/plowtrax/)\n*     New York City, New York: [http://maps.nyc.gov/snow/# ](http://maps.nyc.gov/snow/#%20)\n*     Pennsylvania: [https://www.511pa.com/](https://www.511pa.com/)\n\nBefore using a potentially expensive off-the-shelf commercial solution, we wanted to explore the potential of building a product ourselves. Going this route allowed experimentation and familiarization with cloud computing services that have become an integral part of technology infrastructure in the 21st century. If in the future a commercial product were to arise that had economic and feature benefits it would be considered. \n\nOver the course of the latter half of the 2018 I proposed a system to tackle this challenge. It would talk to tracking devices on our snow plows, understand where they are, and relay that information to a website so anyone could see when their street was plowed. The idea was simple, but the execution of it was complex.\n\n## Timeline\n\nThe creation of the mapping software took the form of three parts: Planning, Development, and Deployment. Planning was a 3.5 month process, while Development took 2.5 months, and deployment is ongoing. The following post details the first iteration of the program.\n\nThe proof of concept had 2 key parts: \n\n1. Operate in near real time:  showing streets being updated in 1 to 10 minute intervals. This posed the challenge of requiring specialized software to operate a geofencing server. The server would store geofences and be able to associate actions with them (i.e., if a truck enters the area, send a message with truck number). \n2. The application had to be light on the user’s browser. Initial testing produced 10-20 megabyte web pages that would throw a “Page Unresponsive” error (Chrome, Firefox, Edge) on computers. This problem would be solved through using minified JSON files (as small as a JSON file can get) as data sources for a lightweight client to load and then store into memory, reducing the resources required for initial access. \n\n## Planning\n\nPlanning was a two-part process: Assessment and GIS research. Assessment started with understanding the capabilities of the Application Programming Interface (APIs) to which I had access. The first of those was our vehicle tracking API. The vehicle tracking API returns City vehicle locations as GPS coordinates: latitude, longitude. Translating those coordinates into a city street block is a two step problem. Using geofences, a polygon of coordinates that have an action attached to them, I was able to generate a geofence per city block and track when a vehicle encountered that fence.\n\n\u003CAstroImage src=\"assets/blog/Whiteboard2.png\" alt=\"Whiteboard covered with various handwritten technical notes and diagrams in multiple colors including pink, orange, green, blue, and black. The content includes flowcharts, database and REST API schemas, server write capacity calculations, potential error handling questions, and a folder structure for a mapping application. There are also timelines and data flow paths sketched out. On the right side, part of a map and a photo of smiling children in a blue holder are visible.\" width={2000} height={972} />\n\n\u003CAstroImage src=\"assets/blog/Whiteboard3.png\" alt=\"Alt text: Whiteboard filled with a mix of hand-drawn maps, tables, and notes in various colors including black, red, blue, green, orange, and purple. A grid-based street map is sketched prominently in black with labeled roads and time annotations. There's a table on the left with columns labeled &#x22;Truck ID&#x22; and &#x22;Time,&#x22; and another table at the bottom listing violations by date. Annotations include terms like “Driving,” “Salt,” “Plow,” and “Salt+Plow” in blue, and phrases such as “join to street ID,” “Fishnet,” and “Display street grid.” The content suggests traffic, vehicle tracking, and geospatial analysis concepts.\" width={2000} height={972} />\n\n## Development\n\nThe development process took the form of a modified agile workflow with small proof of concepts being produced and eventually combined for a larger product. The development process was also an experiment for using Amazon’s AWS(EC2) as a development environment alongside using other Amazon services for the stack: DynamoDB(NoSQL) for the database and S3 for static site hosting (Like Google Drive, or GitHub). \n\n\u003CAstroImage src=\"assets/blog/Screenshot 2025-09-15 at 2.21.30 PM.png\" alt=\"AWS Product icons \" width={1198} height={332} />\n\nThe first step using the City Streets 2011 KML (file format used to display geographic data) found at [data.syr.gov](https://data.syr.gov) was to convert the four corner street addresses of a block for a CITYST\\_ID block into usable geocoordinates (geocoding). This created 4 unique points that could be combined to form a polygon shape for a specific city block. Then those pairs of points would be stored in arrays per CITYST\\_ID.\n\n\u003CAstroImage src=\"assets/blog/WadsworthStTable.png\" alt=\"Wadsworth ST in a table\" width={1702} height={741} />\n\nThis process allowed the creation of visualizations of the geofences and filtering of them before they were implemented into a larger program. The program was designed so that the geofences were fed into a Geofencing server, [Tile38](https://tile38.com/?ref=edwarddeaver.me), and then sent to a message broker, [RabbitMQ](https://www.rabbitmq.com/?ref=edwarddeaver.me). From there, a Python program would listen to the broker to make updates to a DynamoDB database. \n\n\u003CAstroImage src=\"assets/blog/FilteredGeofences.png\" alt=\"Filtering geofences\" width={2000} height={506} />\n\nIn order for the system to map the street data coming from the server-side messages to the front end, the system must establish a universal primary key: CITYST\\_ID from the initial KML file. Within the DynamoDB database, the live street data had its own table with the primary key of CITYST\\_ID. The idea was Tile38 would have webhooks named after the CITYST\\_ID of the geofence they represented. So when the webhook was triggered the system would know a vehicle was in that geofence.\n\nAnother part was getting the data from the City’s Snow & Ice vehicles (this includes snow plows). This was fulfilled by a Python function that called all of the snow plow trucks, stored their data into an array, then created a minified JSON file of that data, and transferred it to the S3 bucket (similar to a folder in Google Drive). The client browser would load that and then store it in memory, reducing the resources required for initial access.\n\n```json\n{\"@type\": \"PagedGpsMessageResult\", \"index\": 0, \"limit\": 1000, \"count\": 121, \"total\": 121, \"gpsMessage\": [{\"messageTime\": \"2018-10-12T14:31:36Z\", \"satellite\": false, \"latitude\": 43.055431, \"longitude\": -76.108658, \"accuracy\": {\"@units\": \"MILES\", \"value\": 0.006213711922373339}, \"odometer\": {\"@units\": \"MILES\", \"@timestamp\": \"2018-10-12T14:31:36Z\", \"value\": 2079.85}, \"keyOn\": false, \"parked\": false, \"lastSpeed\": 0, \"avgSpeed\": 0, \"maxSpeed\": 0, \"vehicleId\": 951175}, {\"messageTime\": \"2018-10-12T15:06:51Z\", \"satellite\": false, \"latitude\": 43.055716, \"longitude\": -76.10752, \"accuracy\": {\"@units\": \"MILES\", \"value\": 0.006213711922373339}, \"odometer\": {\"@units\": \"MILES\", \"@timestamp\": \"2018-10-12T15:06:51Z\", \"value\": 94543.3}, \"keyOn\": false, \"parked\": false, \"lastSpeed\": 0, \"avgSpeed\": 0, \"maxSpeed\": 0, \"vehicleId\": 955670}}]}\n```\n\nAt one-minute intervals, the server-side portion of the system would call the Vehicle Tracking API to retrieve the location of the Snow & Ice vehicles. Once the data is retrieved, the VehicleID, Latitude, Longitude, MessageTime\\*, and currentEpochTime\\*\\* for each vehicle would be uploaded to the Historical Vehicle Location table in our DynamoDB instance to create a historical record that could be reviewed at a later time. At 10 minute intervals the system would upload the API response as a JSON file to the AWS S3 bucket so anyone viewing the website could see where the trucks were at. The front end would load this JSON file and create a Javascript map object (key value pair array) that utilizes the VehicleID as a key. In the map it used this structure \\[ Key: VehicleID, Value: Leaflet Marker Object].\n\nOnce the backend architecture was taking shape, I moved onto the client-side part of the application. The client-side was designed to be light in terms of initial download. This created a few different consequences. While yes, the initial download would be an order of 75% lighter than a folium (leaflet map creation library) HTML file map, it generates all of its objects on the users’ device except for some assets stored on Content Delivery Networks (servers around the globe providing high-speed access to media or other assets). In all tests I conducted this method was faster. Loading a completed map (10mb-20mb file) would crash the Chrome instance I was using, and freeze the machine, because browsers are not designed to download excessively large HTML files, but they do well with gradual streams of data such as media files.\n\nThe front-end javascript loaded vehicle data from a JSON file stored on the S3 instance. The vehicle location data would be stored inside a map structure that associates vehicleID with a leaflet marker object for that ID. Ideally, the program would do that same process for street data, but that did not happen. The application also had a textbox where the user could enter their Syracuse street address, and then it would mark that address on the map, and zoom in; similar to Google Maps. This used the [OpenStreetMap](https://wiki.openstreetmap.org/wiki/API_v0.6?ref=edwarddeaver.me) API to convert an address to geographic coordinates.\n\n## Deployment\n\nFuture me popping into this: hindsight is 2020 and now to deploy a Python application I would use Docker but I didn’t back when this was made. If you decide not to use docker to deploy a Python project deployment becomes difficult as you have to manage dependencies via virtualenv/requirements.txt. Just use Docker. \n\n\u003Ciframe id=\"twitter-widget-0\" scrolling=\"no\" frameborder=\"0\" allowtransparency=\"true\" allowfullscreen=\"true\" class=\"\" style=\"position: static; visibility: visible; width: 550px; height: 464px; display: block; flex-grow: 1;\" title=\"X Post\" src=\"https://platform.twitter.com/embed/Tweet.html?dnt=false&amp;embedId=twitter-widget-0&amp;features=eyJ0ZndfdGltZWxpbmVfbGlzdCI6eyJidWNrZXQiOltdLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X2ZvbGxvd2VyX2NvdW50X3N1bnNldCI6eyJidWNrZXQiOnRydWUsInZlcnNpb24iOm51bGx9LCJ0ZndfdHdlZXRfZWRpdF9iYWNrZW5kIjp7ImJ1Y2tldCI6Im9uIiwidmVyc2lvbiI6bnVsbH0sInRmd19yZWZzcmNfc2Vzc2lvbiI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfZm9zbnJfc29mdF9pbnRlcnZlbnRpb25zX2VuYWJsZWQiOnsiYnVja2V0Ijoib24iLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X21peGVkX21lZGlhXzE1ODk3Ijp7ImJ1Y2tldCI6InRyZWF0bWVudCIsInZlcnNpb24iOm51bGx9LCJ0ZndfZXhwZXJpbWVudHNfY29va2llX2V4cGlyYXRpb24iOnsiYnVja2V0IjoxMjA5NjAwLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X3Nob3dfYmlyZHdhdGNoX3Bpdm90c19lbmFibGVkIjp7ImJ1Y2tldCI6Im9uIiwidmVyc2lvbiI6bnVsbH0sInRmd19kdXBsaWNhdGVfc2NyaWJlc190b19zZXR0aW5ncyI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfdXNlX3Byb2ZpbGVfaW1hZ2Vfc2hhcGVfZW5hYmxlZCI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfdmlkZW9faGxzX2R5bmFtaWNfbWFuaWZlc3RzXzE1MDgyIjp7ImJ1Y2tldCI6InRydWVfYml0cmF0ZSIsInZlcnNpb24iOm51bGx9LCJ0ZndfbGVnYWN5X3RpbWVsaW5lX3N1bnNldCI6eyJidWNrZXQiOnRydWUsInZlcnNpb24iOm51bGx9LCJ0ZndfdHdlZXRfZWRpdF9mcm9udGVuZCI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9fQ%3D%3D&amp;frame=false&amp;hideCard=false&amp;hideThread=false&amp;id=1105094831989313537&amp;lang=en&amp;origin=https%3A%2F%2Fedwarddeaver.me%2Fportfolio%2Fsyracuse-snow-plow-mapping&amp;sessionId=9dc4ace05695f3745de7ce5c0dc7d2c9eec4faeb&amp;theme=light&amp;widgetsVersion=2615f7e52b7e0%3A1702314776716&amp;width=550px\" data-tweet-id=\"1105094831989313537\" tabindex=\"0\">\u003C/iframe>\n\n\u003Ciframe allowfullscreen=\"allowfullscreen\" src=\"https://www.linkedin.com/embed/feed/update/urn:li:share:6511232724378931200?wmode=opaque\" data-embed=\"true\" frameborder=\"0\" style=\"box-sizing: border-box; margin: 0px; padding: 0px; width: 1140px;\" loading=\"lazy\" title=\"Embedded post\" height=\"455\" tabindex=\"0\">\u003C/iframe>\n\n\u003Ciframe id=\"twitter-widget-1\" scrolling=\"no\" frameborder=\"0\" allowtransparency=\"true\" allowfullscreen=\"true\" class=\"\" style=\"position: static; visibility: visible; width: 550px; height: 488px; display: block; flex-grow: 1;\" title=\"X Post\" src=\"https://platform.twitter.com/embed/Tweet.html?dnt=false&amp;embedId=twitter-widget-1&amp;features=eyJ0ZndfdGltZWxpbmVfbGlzdCI6eyJidWNrZXQiOltdLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X2ZvbGxvd2VyX2NvdW50X3N1bnNldCI6eyJidWNrZXQiOnRydWUsInZlcnNpb24iOm51bGx9LCJ0ZndfdHdlZXRfZWRpdF9iYWNrZW5kIjp7ImJ1Y2tldCI6Im9uIiwidmVyc2lvbiI6bnVsbH0sInRmd19yZWZzcmNfc2Vzc2lvbiI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfZm9zbnJfc29mdF9pbnRlcnZlbnRpb25zX2VuYWJsZWQiOnsiYnVja2V0Ijoib24iLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X21peGVkX21lZGlhXzE1ODk3Ijp7ImJ1Y2tldCI6InRyZWF0bWVudCIsInZlcnNpb24iOm51bGx9LCJ0ZndfZXhwZXJpbWVudHNfY29va2llX2V4cGlyYXRpb24iOnsiYnVja2V0IjoxMjA5NjAwLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X3Nob3dfYmlyZHdhdGNoX3Bpdm90c19lbmFibGVkIjp7ImJ1Y2tldCI6Im9uIiwidmVyc2lvbiI6bnVsbH0sInRmd19kdXBsaWNhdGVfc2NyaWJlc190b19zZXR0aW5ncyI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfdXNlX3Byb2ZpbGVfaW1hZ2Vfc2hhcGVfZW5hYmxlZCI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfdmlkZW9faGxzX2R5bmFtaWNfbWFuaWZlc3RzXzE1MDgyIjp7ImJ1Y2tldCI6InRydWVfYml0cmF0ZSIsInZlcnNpb24iOm51bGx9LCJ0ZndfbGVnYWN5X3RpbWVsaW5lX3N1bnNldCI6eyJidWNrZXQiOnRydWUsInZlcnNpb24iOm51bGx9LCJ0ZndfdHdlZXRfZWRpdF9mcm9udGVuZCI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9fQ%3D%3D&amp;frame=false&amp;hideCard=false&amp;hideThread=false&amp;id=1103689969586130946&amp;lang=en&amp;origin=https%3A%2F%2Fedwarddeaver.me%2Fportfolio%2Fsyracuse-snow-plow-mapping&amp;sessionId=9dc4ace05695f3745de7ce5c0dc7d2c9eec4faeb&amp;theme=light&amp;widgetsVersion=2615f7e52b7e0%3A1702314776716&amp;width=550px\" data-tweet-id=\"1103689969586130946\" tabindex=\"0\">\u003C/iframe>","src/content/portfolio/syracuse-snow-plow-mapping.mdx","8d5d749776a6fef4","syracuse-snow-plow-mapping.mdx","is-the-mail-here-yet",{"id":444,"data":446,"body":455,"filePath":456,"digest":457,"legacyId":458,"deferredRender":28},{"title":447,"description":448,"image":449,"date":450,"author":18,"categories":451,"tags":453},"Is The Mail Here Yet?","For Hack Upstate 2020 hackathon, I made a tool that emailed me when my webcam detected a Fedex or UPS truck, so I wouldn't miss package deliveries. Won 2nd place.","assets/blog/AdobeStock_591168933_Preview_Editorial_Use_Only.png",["Date","2020-10-01T04:00:00.000Z"],[452],"Computer Vision",[454,86,73],"opencv","For Hack Upstate 2020 hackathon, I made a tool that emailed me when my webcam detected a Fedex or UPS truck, so I wouldn't miss package deliveries. Won 2nd place.\n\n\u003CVimeo id=\"867673470\" />\n\nFor my Hack Upstate 2020 submission (a 24 hour hackathon), I created a tool that emailed me when a Fedex or UPS truck was detected from my webcam. The goal was to not miss when a mail truck delivered a package. I received 2nd place.\n\nIt utilized [Google's Teachable Machine](https://teachablemachine.withgoogle.com/?ref=edwarddeaver.me) to train the model and TensorFlow/OpenCV to detect the trucks.\n\nSome challenges I ran into were:\n\n*     CUDA version support on my OS vs TensorFlow.\n*     Trying to ingest an RTMP stream via a Wyze camera resulted in random crashes from inconsistent stream. The RTMP support for Wyze was in beta at the time.\n\nIf you are interested the Github repository can be found here: [https://github.com/EdwardDeaver/HackUpstate2020.](https://github.com/EdwardDeaver/HackUpstate2020)\n\nIf you are interested in the DevPost link, that can be found here: [https://devpost.com/software/is-the-mail-here-yet](https://devpost.com/software/is-the-mail-here-yet).","src/content/portfolio/is-the-mail-here-yet.mdx","f5b9d9a3de684fcf","is-the-mail-here-yet.mdx","city-of-syracuse-video-production",{"id":459,"data":461,"body":470,"filePath":471,"digest":472,"legacyId":473,"deferredRender":28},{"title":462,"description":463,"image":464,"date":465,"author":18,"categories":466,"tags":468},"City of Syracuse Video Production","Videos produced for the City of Syracuse Innovation team.","assets/blog/Screenshot 2025-09-15 at 3.02.22 PM.png",["Date","2019-06-30T04:00:00.000Z"],[467],"video-production",[20,469],"adobe premiere","StarTup In Residence Startup in Residence is a program that connects governments with startups to solve government-led civic challenges.\n\nWhile at the city of Syracuse I interned as a Storytelling Intern.\n\n## StarTup In Residence\n\nStartup in Residence is a program that connects governments with startups to solve government-led civic challenges. As a government partner in the 2018-2019 STiR cohort, the City of Syracuse has identified three different City challenges that we have three different start-up companies helping us to solve. In this video is Adria Finch(former Chief Innovation Office for the City of Syracuse).\n\n\u003CYouTube id=\"_N1odygmoHo\" />\n\n### Camino\n\nCity of Syracuse worked with Camino to provide a better permitting process. In the video is Jake Dishaw (Director of Central Permit Office at City of Syracuse) and Nate Levine (Founder at Camino).\n\n\u003CYouTube id=\"D1XhlPGs8vw\" />\n\n### Vite\n\nThe City of Syracuse worked with Vite to provide a community incentive. In the video is Samantha Linnett (former Innovation Design Lead for the Office of Accountability Performance and Innovation at the City of Syracuse) and Luke Kim(Chief Marketing Officer of VITE Labs).\n\n\u003CYouTube id=\"VAcWxiyNl7o\" />\n\n### Zivics\n\nIn the video is Timothy \"Noble\" Jennings-Bey (Director of the Trauma Response Team) and Adam Peruta (President of Zivics).\n\n\u003CYouTube id=\"y2qdASz4lHk\" />\n\n## 2019 Priority Area: Permitting\n\n\"When people think about neighborhood development and economic development, they often think of construction, new buildings, renovations, small businesses, and jobs.  However, they don’t often think about the behind the scenes work that goes into those larger efforts.  People spend countless hours planning, working, and developing their projects.  They spend time and money to create something that will not only impact their lives, but the entire community.\" [http://www.innovatesyracuse.com/blog/permitting](http://www.innovatesyracuse.com/blog/permitting)\n\n\u003CYouTube id=\"s9yef0qiT4M\" />","src/content/portfolio/city-of-syracuse-video-production.mdx","38b0a5ca65ccc9be","city-of-syracuse-video-production.mdx"]